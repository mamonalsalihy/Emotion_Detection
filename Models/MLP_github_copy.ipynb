{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLP_github_copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b25d129d36464824b1ecc7a864448c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e725e2008b194a6c8ea4c8a51cc5e2cc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0550471d20a64b269e9f7dc9200e5f44",
              "IPY_MODEL_7cbb5f9108324a94938331abc625e534"
            ]
          }
        },
        "e725e2008b194a6c8ea4c8a51cc5e2cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0550471d20a64b269e9f7dc9200e5f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f1ce600d3c9048e1be97c0fedcd8973e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1034ba5e793340cfb8b3dfcab83c7688"
          }
        },
        "7cbb5f9108324a94938331abc625e534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_718f23865ab74a3fa872b09793ea1d9d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 704/? [00:19&lt;00:00, 38.42it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7dd1935cfdf45acbab35a037566ac5e"
          }
        },
        "f1ce600d3c9048e1be97c0fedcd8973e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1034ba5e793340cfb8b3dfcab83c7688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "718f23865ab74a3fa872b09793ea1d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7dd1935cfdf45acbab35a037566ac5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamonalsalihy/Emotion_Detection/blob/main/Models/MLP_github_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W8GiIqkZLtC",
        "outputId": "a45b55d9-9fdd-463a-fe47-189c3194d0c3"
      },
      "source": [
        "# loading in the dependencies\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import pickle as pkl\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sea\n",
        "import re\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import math\n",
        "from itertools import chain\n",
        "\n",
        "import gensim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DtZFtAg7uga"
      },
      "source": [
        "# Must upload the dataset splits to session to read the data.\r\n",
        "test_path = './test.csv'\r\n",
        "train_path = './train.csv'\r\n",
        "valid_path = './valid.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbS1su3ry8nR",
        "outputId": "032765ce-9fd2-469f-918e-b8a1a3cea947"
      },
      "source": [
        "with open(train_path, \"r\") as intrain: \n",
        "    train = pd.read_csv(intrain)\n",
        "with open(valid_path, \"r\") as indev: \n",
        "    valid = pd.read_csv(indev)\n",
        "with open(test_path, \"r\") as intest: \n",
        "    test = pd.read_csv(intest)\n",
        "print(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    conv_id  utterance_idx  ... Sleeping Grooming\n",
            "0              hit:0_conv:1              1  ...        0        0\n",
            "1              hit:0_conv:1              2  ...        0        0\n",
            "2              hit:0_conv:1              3  ...        0        0\n",
            "3              hit:0_conv:1              4  ...        0        0\n",
            "4              hit:0_conv:1              5  ...        0        0\n",
            "...                     ...            ...  ...      ...      ...\n",
            "76663  hit:12424_conv:24848              5  ...        0        0\n",
            "76664  hit:12424_conv:24849              1  ...        0        0\n",
            "76665  hit:12424_conv:24849              2  ...        0        0\n",
            "76666  hit:12424_conv:24849              3  ...        0        0\n",
            "76667  hit:12424_conv:24849              4  ...        0        0\n",
            "\n",
            "[76668 rows x 93 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AHCr1-LE2PA"
      },
      "source": [
        "##Creating Speaker and Listener "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dod-Bn41htrq"
      },
      "source": [
        "train_speaker = train.loc[train[\"speaker_label\"] == \"speaker\"]\n",
        "train_listener = train.loc[train[\"speaker_label\"] == \"listener\"]\n",
        "\n",
        "valid_speaker = valid.loc[valid[\"speaker_label\"] == \"speaker\"]\n",
        "valid_listener = valid.loc[valid[\"speaker_label\"] == \"listener\"]\n",
        "\n",
        "test_speaker = test.loc[test[\"speaker_label\"] == \"speaker\"]\n",
        "test_listener = test.loc[test[\"speaker_label\"] == \"listener\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ApvqMmyZf9"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5S8ZV6hzquO"
      },
      "source": [
        "def convert_column(dataset, column): \n",
        "   texts = [text for text in dataset[column]]\n",
        "   texts = (*texts,)\n",
        "   return texts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMEiQ4T4Hg6N"
      },
      "source": [
        "# ONLY PROMPTS\n",
        "train = train.drop_duplicates(subset=['clean_prompt'])\n",
        "valid = valid.drop_duplicates(subset=['clean_prompt'])\n",
        "#test = test.drop_duplicates(subset=['clean_prompt'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEjlHcy7-pB_"
      },
      "source": [
        "def convert_column(dataset, column): \n",
        "   texts = [text for text in dataset[column]]\n",
        "   texts = (*texts,)\n",
        "   return texts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfvQIXRLaynC"
      },
      "source": [
        "train_liwc = train[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "valid_liwc = valid[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "test_liwc = test[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "\n",
        "# utterance features\n",
        "train_utterances = convert_column(train,'clean_utterance')\n",
        "valid_utterances = convert_column(valid,'clean_utterance')\n",
        "test_utterances = convert_column(test,'clean_utterance')\n",
        "\n",
        "\n",
        "# prompts features\n",
        "train_prompt = convert_column(train,'clean_prompt')\n",
        "valid_prompt = convert_column(valid,'clean_prompt')\n",
        "test_prompt = convert_column(test,'clean_prompt')\n",
        "\n",
        "\n",
        "# emotion labels 32\n",
        "train_context_labels = convert_column(train,'context')\n",
        "valid_context_labels = convert_column(valid,'context')\n",
        "test_context_labels = convert_column(test,'context')\n",
        "\n",
        "# emotion labels 3\n",
        "train_category_labels = convert_column(train,'emotion_category')\n",
        "valid_category_labels = convert_column(train,'emotion_category')\n",
        "test_category_labels = convert_column(train,'emotion_category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR4HI3VSwdIJ"
      },
      "source": [
        "##Feature Builder Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "545JwQedwe-a"
      },
      "source": [
        "# We'll be using SkLearn's TfIdfVectorizer to construct our n-gram feature vectors\n",
        "\n",
        "# We'll create our custom sequencer class for converting a text into a sequence of integers corresponding to our tokens\n",
        "class Sequencer(object):\n",
        "    def __init__(self, corpus, bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>'):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "\n",
        "        self.unk_index = self.add_token(unk_token) \n",
        "        self.pad_index = self.add_token(pad_token)\n",
        "        self.bos_index = self.add_token(bos_token)\n",
        "        self.eos_index = self.add_token(eos_token)\n",
        "        self.nlp = spacy.load('en')\n",
        "        self.tokenizer = lambda text: [t.text for t in self.nlp(text)]\n",
        "\n",
        "    def add_token(self, token):\n",
        "\n",
        "        self.word2idx[token] = new_index = len(self.word2idx)\n",
        "        self.idx2word[new_index] = token\n",
        "\n",
        "        return new_index\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Input will look like:\n",
        "        # [<s>, w1, w2, ..., wn, </s>]\n",
        "        tokens = self.tokenizer(text)\n",
        "\n",
        "        sequence = [self.bos_index]\n",
        "        for token in tokens:\n",
        "\n",
        "            index = self.word2idx.get(token, self.unk_index)\n",
        "            sequence.append(index)\n",
        "        sequence.append(self.eos_index)\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def create_padded_tensor(self, sequences):\n",
        "        # Given a list of sequences, pad all to the same length\n",
        "\n",
        "        max_seq_len = max(len(sequence) for sequence in sequences)\n",
        "        tensor = torch.full((len(sequences), max_seq_len), self.pad_index, dtype=torch.long)\n",
        "\n",
        "        for i, sequence in enumerate(sequences):\n",
        "            for j, token in enumerate(sequence):\n",
        "                tensor[i][j] = token\n",
        "        \n",
        "        return tensor\n",
        "\n",
        "# For converting labels into indices\n",
        "class LabelIndexer(object):\n",
        "    def __init__(self, labels):\n",
        "        self.label2idx = {label: i for i, label in enumerate(labels)}\n",
        "        self.idx2label = {i:label for label, i in self.label2idx.items()}\n",
        "        self.labels = labels\n",
        "\n",
        "    def encode(self, y):\n",
        "        return self.label2idx[y]\n",
        "\n",
        "    def encode_batch(self, ys):\n",
        "        return torch.LongTensor([self.encode(y) for y in ys])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMi7614Zwf7O"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4juajTNCxYlm"
      },
      "source": [
        "class EmpatheticDataset(Dataset):\n",
        "    def __init__(self, texts, liwc, labels, input_transformer, output_transformer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.liwc  = liwc.values\n",
        "        self.input_transformer = input_transformer\n",
        "        self.output_transformer = output_transformer\n",
        "\n",
        "    def __getitem__(self, index): # Return a single example\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "        liwc = self.liwc[index]\n",
        "        x_liwc = torch.tensor(liwc)\n",
        "        x = self.input_transformer(text)\n",
        "        y = self.output_transformer(label)       \n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9hcGh-bZLJK"
      },
      "source": [
        "class EmpatheticDatasetLIWC(Dataset):\r\n",
        "    def __init__(self, texts, liwc, labels, input_transformer, output_transformer):\r\n",
        "        self.texts = texts\r\n",
        "        self.labels = labels\r\n",
        "        self.liwc  = liwc.values\r\n",
        "        self.input_transformer = input_transformer\r\n",
        "        self.output_transformer = output_transformer\r\n",
        "\r\n",
        "    def __getitem__(self, index): # Return a single example\r\n",
        "        text = self.texts[index]\r\n",
        "        label = self.labels[index]\r\n",
        "        liwc = self.liwc[index]\r\n",
        "        x_liwc = torch.tensor(liwc)\r\n",
        "        x = self.input_transformer(text)\r\n",
        "        y = self.output_transformer(label)       \r\n",
        "        return x_liwc.float(), y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqL1cJSCvksw"
      },
      "source": [
        "### MultiLayer Perceptron\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU4-LPelxufo"
      },
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    At its simplest, a multilayer perceptron is a 2 layer network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout=False, dropout_p=0.1):\n",
        "        super(MultiLayerPerceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size, bias=True)\n",
        "\n",
        "        self.add_dropout = dropout\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        if self.add_dropout:\n",
        "            logits = self.fc2(self.dropout(h))\n",
        "        else:\n",
        "            logits = self.fc2(h)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb728zcXx2Nd"
      },
      "source": [
        "## Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iTOIdN2x5vv"
      },
      "source": [
        "class MultiClassTrainer(object):\n",
        "    \"\"\"\n",
        "    Trainer for training a multi-class classification model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, device=\"cpu\", log_every_n=None):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        self.log_every_n = log_every_n if log_every_n else 0\n",
        "\n",
        "\n",
        "    def _print_summary(self):\n",
        "        print(self.model)\n",
        "        print(self.optimizer)\n",
        "        print(self.loss_fn)\n",
        "\n",
        "    def train(self, loader):\n",
        "        \"\"\"\n",
        "        Run a single epoch of training\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.train() # Run model in training mode\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        for i, batch in tqdm(enumerate(loader)):\n",
        "            batch_size = batch[0].shape[0]\n",
        "            self.optimizer.zero_grad() # Always set gradient to 0 before computing it\n",
        "\n",
        "            logits = self.model(batch[0].to(self.device)) # Forward pass, # Wx + b\n",
        "            #print(logits)\n",
        "            #print(batch[1].view(-1).to(self.device))\n",
        "            loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss: Cross entropy loss\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            \n",
        "\n",
        "            running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "\n",
        "            if self.log_every_n and i % self.log_every_n == 0:\n",
        "                print(\"Running loss: \", running_loss)\n",
        "\n",
        "            running_loss_history.append(running_loss)\n",
        "\n",
        "            loss.backward() # Perform backprop, which will compute dL/dw\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 3.0)\n",
        "            self.optimizer.step() # Update step: w = w - eta * dL / dW\n",
        "\n",
        "        print(\"Epoch completed!\")\n",
        "        print(\"Epoch Loss: \", running_loss)\n",
        "        print(\"Epoch Perplexity: \", math.exp(running_loss))\n",
        "\n",
        "        # The history information can allow us to draw a loss plot\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def evaluate(self, loader, labels):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a validation set\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval() # Run model in eval mode (disables dropout layer)\n",
        "\n",
        "        batch_wise_true_labels = []\n",
        "        batch_wise_predictions = []\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient computation - required only during training\n",
        "            for i, batch in tqdm(enumerate(loader)):\n",
        "                # batch[0] shape: (batch_size, input_size)\n",
        "\n",
        "                logits = self.model(batch[0].to(self.device)) # Run forward pass (except we don't store gradients)\n",
        "                # logits shape: (batch_size, num_classes)\n",
        "                \n",
        "                loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss\n",
        "                # No backprop is done during validation\n",
        "                loss_history.append(loss.item())\n",
        "\n",
        "                running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "                \n",
        "                running_loss_history.append(running_loss)\n",
        "\n",
        "                # Converts the raw outputs into probabilities for each class using softmax\n",
        "                probs = F.softmax(logits, dim=-1) \n",
        "                # probs shape: (batch_size, num_classes)\n",
        "\n",
        "                predictions = torch.argmax(probs, dim=-1) # Output predictions\n",
        "                # predictions shape: (batch_size)\n",
        "\n",
        "                batch_wise_true_labels.append(batch[1].tolist())\n",
        "                batch_wise_predictions.append(predictions.tolist())\n",
        "        \n",
        "        # flatten the list of predictions using itertools\n",
        "        all_true_labels = list(chain.from_iterable(batch_wise_true_labels))\n",
        "        all_predictions = list(chain.from_iterable(batch_wise_predictions))\n",
        "\n",
        "        # Now we can generate a classification report\n",
        "        print(\"Classification report after epoch:\")\n",
        "        print(classification_report(all_true_labels, all_predictions))\n",
        "        # print(confusion_matrix(all_true_labels,all_predictions))\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def get_model_dict(self):\n",
        "        return self.model.state_dict()\n",
        "\n",
        "    def run_training(self, train_loader, valid_loader, labels, n_epochs=10):\n",
        "        # Useful for us to review what experiment we're running\n",
        "        # Normally, you'd want to save this to a file\n",
        "        self._print_summary()\n",
        "\n",
        "        train_losses = []\n",
        "        train_running_losses = []\n",
        "\n",
        "        valid_losses = []\n",
        "        valid_running_losses = []\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            loss_history, running_loss_history = self.train(train_loader)\n",
        "            valid_loss_history, valid_running_loss_history = self.evaluate(valid_loader, labels)\n",
        "\n",
        "            train_losses.append(loss_history)\n",
        "            train_running_losses.append(running_loss_history)\n",
        "\n",
        "            valid_losses.append(valid_loss_history)\n",
        "            valid_running_losses.append(valid_running_loss_history)\n",
        "\n",
        "        # Training done, let's look at the loss curves\n",
        "        all_train_losses = list(chain.from_iterable(train_losses))\n",
        "        all_train_running_losses = list(chain.from_iterable(train_running_losses))\n",
        "\n",
        "        all_valid_losses = list(chain.from_iterable(valid_losses))\n",
        "        all_valid_running_losses = list(chain.from_iterable(valid_running_losses))\n",
        "\n",
        "        train_epoch_idx = range(len(all_train_losses))\n",
        "        valid_epoch_idx = range(len(all_valid_losses))\n",
        "        # sns.lineplot(epoch_idx, all_losses)\n",
        "        sns.lineplot(train_epoch_idx, all_train_running_losses)\n",
        "        sns.lineplot(valid_epoch_idx, all_valid_running_losses)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dERntpBGZ3dd"
      },
      "source": [
        "##Prompts Dataloader w/ 32 emotion labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3tCwpKqyZ0u"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer()\n",
        "tfidf_vec.fit(train_prompt)\n",
        "input_transformer = lambda text: torch.FloatTensor(tfidf_vec.transform([text]).todense()).squeeze(0)\n",
        "\n",
        "label_indexer = LabelIndexer(list(set(train_context_labels + valid_context_labels)))\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\n",
        "\n",
        "train_tfidf_dataset = EmpatheticDataset(train_prompt, train_liwc, train_context_labels, input_transformer, output_transformer)\n",
        "valid_tfidf_dataset = EmpatheticDataset(valid_prompt, valid_liwc,  valid_context_labels, input_transformer, output_transformer)\n",
        "test_tfidf_dataset = EmpatheticDataset(test_prompt, test_liwc, test_context_labels, input_transformer, output_transformer)\n",
        "\n",
        "train_tfidf_loader = torch.utils.data.DataLoader(train_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "valid_tfidf_loader = torch.utils.data.DataLoader(valid_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "test_tfidf_loader = torch.utils.data.DataLoader(test_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCzU8a_7Z-nX"
      },
      "source": [
        "##Utterance Dataloader w/ 32 Emotion Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWXldn6wZ_An"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer()\n",
        "tfidf_vec.fit(train_prompt)\n",
        "input_transformer = lambda text: torch.FloatTensor(tfidf_vec.transform([text]).todense()).squeeze(0)\n",
        "\n",
        "label_indexer = LabelIndexer(list(set(train_context_labels + valid_context_labels)))\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\n",
        "\n",
        "train_tfidf_dataset = EmpatheticDataset(train_prompt, train_liwc, train_context_labels, input_transformer, output_transformer)\n",
        "valid_tfidf_dataset = EmpatheticDataset(valid_utterances, valid_liwc,  valid_context_labels, input_transformer, output_transformer)\n",
        "test_tfidf_dataset = EmpatheticDataset(test_utterances, test_liwc, test_context_labels, input_transformer, output_transformer)\n",
        "\n",
        "train_tfidf_loader = torch.utils.data.DataLoader(train_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "valid_tfidf_loader = torch.utils.data.DataLoader(valid_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "test_tfidf_loader = torch.utils.data.DataLoader(test_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "# train_sequence_loader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=16)\n",
        "# valid_sequence_loader = torch.utils.data.DataLoader(valid_seq_dataset, batch_size=16)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7TAIpboaITh"
      },
      "source": [
        "##LIWC Dataloader w/ 32 emotion labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjtepuL3aRMd"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer()\n",
        "tfidf_vec.fit(train_prompt)\n",
        "input_transformer = lambda text: torch.FloatTensor(tfidf_vec.transform([text]).todense()).squeeze(0)\n",
        "\n",
        "label_indexer = LabelIndexer(list(set(train_context_labels + valid_context_labels)))\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\n",
        "\n",
        "train_tfidf_dataset = EmpatheticDatasetLIWC(train_prompt, train_liwc, train_context_labels, input_transformer, output_transformer)\n",
        "valid_tfidf_dataset = EmpatheticDatasetLIWC(valid_prompt, valid_liwc,  valid_context_labels, input_transformer, output_transformer)\n",
        "test_tfidf_dataset = EmpatheticDatasetLIWC(test_prompt, test_liwc, test_context_labels, input_transformer, output_transformer)\n",
        "\n",
        "train_tfidf_loader = torch.utils.data.DataLoader(train_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "valid_tfidf_loader = torch.utils.data.DataLoader(valid_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "test_tfidf_loader = torch.utils.data.DataLoader(test_tfidf_dataset, batch_size=16, shuffle=True)\n",
        "# train_sequence_loader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=16)\n",
        "# valid_sequence_loader = torch.utils.data.DataLoader(valid_seq_dataset, batch_size=16)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1BKZxJvdkdp"
      },
      "source": [
        "##Prompts Dataloader w/ 3 Emotion Groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifGfPTy_dj9V"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer()\r\n",
        "tfidf_vec.fit(train_prompt)\r\n",
        "input_transformer = lambda text: torch.FloatTensor(tfidf_vec.transform([text]).todense()).squeeze(0)\r\n",
        "\r\n",
        "label_indexer = LabelIndexer(list(set(train_category_labels + valid_category_labels)))\r\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\r\n",
        "\r\n",
        "train_tfidf_dataset = EmpatheticDataset(train_prompt, train_liwc, train_category_labels, input_transformer, output_transformer)\r\n",
        "valid_tfidf_dataset = EmpatheticDataset(valid_prompt, valid_liwc,  valid_category_labels, input_transformer, output_transformer)\r\n",
        "test_tfidf_dataset = EmpatheticDataset(test_prompt, test_liwc, test_category_labels, input_transformer, output_transformer)\r\n",
        "\r\n",
        "train_tfidf_loader = torch.utils.data.DataLoader(train_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "valid_tfidf_loader = torch.utils.data.DataLoader(valid_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "test_tfidf_loader = torch.utils.data.DataLoader(test_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "# train_sequence_loader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=16)\r\n",
        "# valid_sequence_loader = torch.utils.data.DataLoader(valid_seq_dataset, batch_size=16)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CAXsC1Ddjt6"
      },
      "source": [
        "##Utterance Dataloader w/ 3 Emotion Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM27mAgZgAOB"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer()\r\n",
        "tfidf_vec.fit(train_prompt)\r\n",
        "input_transformer = lambda text: torch.FloatTensor(tfidf_vec.transform([text]).todense()).squeeze(0)\r\n",
        "\r\n",
        "label_indexer = LabelIndexer(list(set(train_category_labels + valid_category_labels)))\r\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\r\n",
        "\r\n",
        "train_tfidf_dataset = EmpatheticDataset(train_prompt, train_liwc, train_category_labels, input_transformer, output_transformer)\r\n",
        "valid_tfidf_dataset = EmpatheticDataset(valid_utterances, valid_liwc,  valid_category_labels, input_transformer, output_transformer)\r\n",
        "test_tfidf_dataset = EmpatheticDataset(test_utterances, test_liwc, test_category_labels, input_transformer, output_transformer)\r\n",
        "\r\n",
        "train_tfidf_loader = torch.utils.data.DataLoader(train_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "valid_tfidf_loader = torch.utils.data.DataLoader(valid_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "test_tfidf_loader = torch.utils.data.DataLoader(test_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "# train_sequence_loader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=16)\r\n",
        "# valid_sequence_loader = torch.utils.data.DataLoader(valid_seq_dataset, batch_size=16)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtvW6ahGgD8o"
      },
      "source": [
        "##LIWC Dataloader w/ 3 Emotion Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "hVPITTBMgXI_",
        "outputId": "c54fe749-2586-4a60-839a-24f271e60c00"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer()\r\n",
        "tfidf_vec.fit(train_prompt)\r\n",
        "input_transformer = lambda text: torch.FloatTensor(tfidf_vec.transform([text]).todense()).squeeze(0)\r\n",
        "\r\n",
        "label_indexer = LabelIndexer(list(set(train_category_labels + valid_category_labels)))\r\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\r\n",
        "\r\n",
        "train_tfidf_dataset = EmpatheticDatasetLIWC(train_prompt, train_liwc, train_category_labels, input_transformer, output_transformer)\r\n",
        "valid_tfidf_dataset = EmpatheticDatasetLIWC(valid_prompt, valid_liwc,  valid_category_labels, input_transformer, output_transformer)\r\n",
        "test_tfidf_dataset = EmpatheticDatasetLIWC(test_prompt, test_liwc, test_category_labels, input_transformer, output_transformer)\r\n",
        "\r\n",
        "train_tfidf_loader = torch.utils.data.DataLoader(train_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "valid_tfidf_loader = torch.utils.data.DataLoader(valid_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "test_tfidf_loader = torch.utils.data.DataLoader(test_tfidf_dataset, batch_size=16, shuffle=True)\r\n",
        "# train_sequence_loader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=16)\r\n",
        "# valid_sequence_loader = torch.utils.data.DataLoader(valid_seq_dataset, batch_size=16)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-b007f3d4f098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlabel_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_grouped_labels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalid_grouped_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moutput_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_indexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_grouped_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCDic_1lyGO2"
      },
      "source": [
        "## Running Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj7HGhE3kqIp"
      },
      "source": [
        "Prompts and Utterances Input Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXtKjPMyyKIm"
      },
      "source": [
        "# Define our experimental setup; we use the same parameters for both MLP models\n",
        "input_size =  len(tfidf_vec.vocabulary_) \n",
        "hidden_size = 200 # An arbitrary hyperparameter we define\n",
        "output_size = len(label_indexer.label2idx)\n",
        "LEARNING_RATE = 1e-2\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGdPGohikyKy"
      },
      "source": [
        "LIWC SIZE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuHKeHxEkx3A"
      },
      "source": [
        "# Define our experimental setup; we use the same parameters for both MLP models\r\n",
        "input_size =  len(train_liwc.columns)\r\n",
        "hidden_size = 200 # An arbitrary hyperparameter we define\r\n",
        "output_size = len(label_indexer.label2idx)\r\n",
        "LEARNING_RATE = 1e-2\r\n",
        "\r\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng9vpuCAmaFG"
      },
      "source": [
        "Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFFyMBsOV5nl"
      },
      "source": [
        "# To print entire confusion matrix\n",
        "import sys\n",
        "import numpy\n",
        "numpy.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306,
          "referenced_widgets": [
            "b25d129d36464824b1ecc7a864448c5d",
            "e725e2008b194a6c8ea4c8a51cc5e2cc",
            "0550471d20a64b269e9f7dc9200e5f44",
            "7cbb5f9108324a94938331abc625e534",
            "f1ce600d3c9048e1be97c0fedcd8973e",
            "1034ba5e793340cfb8b3dfcab83c7688",
            "718f23865ab74a3fa872b09793ea1d9d",
            "a7dd1935cfdf45acbab35a037566ac5e"
          ]
        },
        "id": "c_WGK3wxyTJO",
        "outputId": "c7f34409-1cee-4b3c-cb9a-8cd7acf24e93"
      },
      "source": [
        "# Model 1: MLP\n",
        "mlp = MultiLayerPerceptron(input_size, hidden_size, output_size)\n",
        "#optimizer = optim.SGD(mlp.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "# optimizer = optim.Adam(mlp.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = optim.Adagrad(mlp.parameters(), lr=LEARNING_RATE)\n",
        "mlp_trainer = MultiClassTrainer(mlp, optimizer, loss_fn)\n",
        "mlp_trainer.run_training(train_tfidf_loader, valid_tfidf_loader, label_indexer.labels, n_epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultiLayerPerceptron(\n",
            "  (fc1): Linear(in_features=10205, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "Adagrad (\n",
            "Parameter Group 0\n",
            "    eps: 1e-10\n",
            "    initial_accumulator_value: 0\n",
            "    lr: 0.01\n",
            "    lr_decay: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "CrossEntropyLoss()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b25d129d36464824b1ecc7a864448c5d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7hKTdAYuFOz"
      },
      "source": [
        "#def evaluate(self, loader, labels):\n",
        "mlp_trainer.evaluate(test_tfidf_loader, label_indexer.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXDC2p26f-DT"
      },
      "source": [
        "#def evaluate(self, loader, labels):\n",
        "mlp_trainer2 = MultiClassTrainer(mlp, optimizer, loss_fn)\n",
        "mlp_trainer2.evaluate(test_tfidf_loader, label_indexer.labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}