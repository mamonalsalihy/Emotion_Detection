{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_github_copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e2d8db6b26a24888afcbd24df6e51857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ab8571bfab8f42909115fa57d4ef3ef9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_91d7bd87cffa4ce0806c74def73d246d",
              "IPY_MODEL_8e5ff34fe89e433ba417b02a085e7f00"
            ]
          }
        },
        "ab8571bfab8f42909115fa57d4ef3ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91d7bd87cffa4ce0806c74def73d246d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9cd3eafb08fc4475889b89e160197776",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23c4abf2e66e494ba6efbac7fd1a5ac5"
          }
        },
        "8e5ff34fe89e433ba417b02a085e7f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e30742559c645e9a7081f76aeda168a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 336/? [00:06&lt;00:00, 47.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24e7f0db0d0344bda3cbdc48ee5213ca"
          }
        },
        "9cd3eafb08fc4475889b89e160197776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23c4abf2e66e494ba6efbac7fd1a5ac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e30742559c645e9a7081f76aeda168a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24e7f0db0d0344bda3cbdc48ee5213ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9bacbac970e2488a859a3e49fffcf8cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e0db092d72384b7fa1c498757ef6bb36",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ac9db6ea7db94614b8fe81c64838c416",
              "IPY_MODEL_365e5e0c725c42ed8b866917c248f3bc"
            ]
          }
        },
        "e0db092d72384b7fa1c498757ef6bb36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac9db6ea7db94614b8fe81c64838c416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fa6420fa26434237b85f3b4a27844fea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0855ea2c8334a18b9111efaa4ce9ea1"
          }
        },
        "365e5e0c725c42ed8b866917c248f3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2d320df5c7214b24b6f190b9c8a66bc5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7667/? [01:32&lt;00:00, 83.17it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d306ebba9a0435cb6676fcfefbea0f7"
          }
        },
        "fa6420fa26434237b85f3b4a27844fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0855ea2c8334a18b9111efaa4ce9ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d320df5c7214b24b6f190b9c8a66bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d306ebba9a0435cb6676fcfefbea0f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "964ec6642dbc4b09bae8ac32f3054ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c6990569673d4d74aab580555a077a0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eb93278986be42008a063cdaba5bb778",
              "IPY_MODEL_aa6dbfadbcd14455b9ec222243ecfcb5"
            ]
          }
        },
        "c6990569673d4d74aab580555a077a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb93278986be42008a063cdaba5bb778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1ee50b78245f4acaaafab8a675459fbe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f55cd36d2e644abb92ee5a73a32cb03"
          }
        },
        "aa6dbfadbcd14455b9ec222243ecfcb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00e49a45784e41aa88d0899016989624",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 571/? [00:04&lt;00:00, 137.90it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_798fc14f65fa41ba9080d117bc42aa19"
          }
        },
        "1ee50b78245f4acaaafab8a675459fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f55cd36d2e644abb92ee5a73a32cb03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00e49a45784e41aa88d0899016989624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "798fc14f65fa41ba9080d117bc42aa19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamonalsalihy/Emotion_Detection/blob/main/Models/CNN_github_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W8GiIqkZLtC"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import pickle as pkl\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sea\n",
        "import re\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import math\n",
        "from itertools import chain\n",
        "\n",
        "import gensim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5XyMajwNENj"
      },
      "source": [
        "# Must upload the dataset splits to google session to read the data.\r\n",
        "test_path = './test.csv'\r\n",
        "train_path = './train.csv'\r\n",
        "valid_path = './valid.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLpSB6QKRyt2"
      },
      "source": [
        "with open(train_path, \"r\") as intrain: \n",
        "    train = pd.read_csv(intrain)\n",
        "with open(valid_path, \"r\") as indev: \n",
        "    valid = pd.read_csv(indev)\n",
        "with open(test_path, \"r\") as intest: \n",
        "    test = pd.read_csv(intest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zyaTmOq5KFUf",
        "outputId": "565b1e04-9a40-4ffc-9bad-c28629effad3"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_id</th>\n",
              "      <th>utterance_idx</th>\n",
              "      <th>context</th>\n",
              "      <th>prompt</th>\n",
              "      <th>speaker_idx</th>\n",
              "      <th>utterance</th>\n",
              "      <th>selfeval</th>\n",
              "      <th>tags</th>\n",
              "      <th>clean_prompt</th>\n",
              "      <th>clean_utterance</th>\n",
              "      <th>speaker_label</th>\n",
              "      <th>emotion_category</th>\n",
              "      <th>Total Function Words</th>\n",
              "      <th>Total Pronouns</th>\n",
              "      <th>Personal Pronouns</th>\n",
              "      <th>First Person Singular</th>\n",
              "      <th>First Person Plural</th>\n",
              "      <th>Second Person</th>\n",
              "      <th>Third Person Singular</th>\n",
              "      <th>Third Person Plural</th>\n",
              "      <th>Impersonal Pronouns</th>\n",
              "      <th>Articles</th>\n",
              "      <th>Common Verbs</th>\n",
              "      <th>Auxiliary Verbs</th>\n",
              "      <th>Past Tense</th>\n",
              "      <th>Present Tense</th>\n",
              "      <th>Future Tense</th>\n",
              "      <th>Adverbs</th>\n",
              "      <th>Prepositions</th>\n",
              "      <th>Conjunctions</th>\n",
              "      <th>Negations</th>\n",
              "      <th>Quantifiers</th>\n",
              "      <th>Number</th>\n",
              "      <th>Swear Words</th>\n",
              "      <th>Social Processes</th>\n",
              "      <th>Family</th>\n",
              "      <th>Friends</th>\n",
              "      <th>Humans</th>\n",
              "      <th>Affective Processes</th>\n",
              "      <th>Positive Emotion</th>\n",
              "      <th>...</th>\n",
              "      <th>Perceptual Processes</th>\n",
              "      <th>See</th>\n",
              "      <th>Hear</th>\n",
              "      <th>Feel</th>\n",
              "      <th>Biological Processes</th>\n",
              "      <th>Body</th>\n",
              "      <th>Health</th>\n",
              "      <th>Sexual</th>\n",
              "      <th>Ingestion</th>\n",
              "      <th>Relativity</th>\n",
              "      <th>Motion</th>\n",
              "      <th>Space</th>\n",
              "      <th>Time</th>\n",
              "      <th>Work</th>\n",
              "      <th>Achievement</th>\n",
              "      <th>Leisure</th>\n",
              "      <th>Home</th>\n",
              "      <th>Money</th>\n",
              "      <th>Religion</th>\n",
              "      <th>Death</th>\n",
              "      <th>Assent</th>\n",
              "      <th>Nonfluencies</th>\n",
              "      <th>Fillers</th>\n",
              "      <th>Total first person</th>\n",
              "      <th>Total third person</th>\n",
              "      <th>Positive feelings</th>\n",
              "      <th>Optimism and energy</th>\n",
              "      <th>Communication</th>\n",
              "      <th>Other references to people</th>\n",
              "      <th>Up</th>\n",
              "      <th>Down</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>School</th>\n",
              "      <th>Sports</th>\n",
              "      <th>TV</th>\n",
              "      <th>Music</th>\n",
              "      <th>Metaphysical issues</th>\n",
              "      <th>Physical states and functions</th>\n",
              "      <th>Sleeping</th>\n",
              "      <th>Grooming</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>1</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>1</td>\n",
              "      <td>I remember going to see the fireworks with my ...</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>i remember going to see the fireworks with my ...</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>2</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>0</td>\n",
              "      <td>Was this a friend you were in love with_comma_...</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>was this a friend you were in love with or jus...</td>\n",
              "      <td>listener</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>3</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>1</td>\n",
              "      <td>This was a best friend. I miss her.</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>this was a best friend. i miss her.</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>4</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>0</td>\n",
              "      <td>Where has she gone?</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>where has she gone?</td>\n",
              "      <td>listener</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>5</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>1</td>\n",
              "      <td>We no longer talk.</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>we no longer talk.</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76663</th>\n",
              "      <td>hit:12424_conv:24848</td>\n",
              "      <td>5</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I found some pictures of my grandma in the att...</td>\n",
              "      <td>389</td>\n",
              "      <td>Yeah reminds me of the good old days.  I miss ...</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i found some pictures of my grandma in the att...</td>\n",
              "      <td>yeah reminds me of the good old days.  i miss ...</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76664</th>\n",
              "      <td>hit:12424_conv:24849</td>\n",
              "      <td>1</td>\n",
              "      <td>surprised</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>294</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>38.461538</td>\n",
              "      <td>30.769231</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76665</th>\n",
              "      <td>hit:12424_conv:24849</td>\n",
              "      <td>2</td>\n",
              "      <td>surprised</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>389</td>\n",
              "      <td>Oh hey that's awesome!  That is awesome right?</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>oh hey that's awesome!  that is awesome right?</td>\n",
              "      <td>listener</td>\n",
              "      <td>1</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>38.461538</td>\n",
              "      <td>30.769231</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76666</th>\n",
              "      <td>hit:12424_conv:24849</td>\n",
              "      <td>3</td>\n",
              "      <td>surprised</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>294</td>\n",
              "      <td>It is soooo awesome.  We have been wanting a b...</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>it is soooo awesome.  we have been wanting a b...</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>38.461538</td>\n",
              "      <td>30.769231</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76667</th>\n",
              "      <td>hit:12424_conv:24849</td>\n",
              "      <td>4</td>\n",
              "      <td>surprised</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>389</td>\n",
              "      <td>That is awesome!!!! Congratulations!</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>that is awesome!!!! congratulations!</td>\n",
              "      <td>listener</td>\n",
              "      <td>1</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>38.461538</td>\n",
              "      <td>30.769231</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76668 rows Ã— 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    conv_id  utterance_idx  ... Sleeping Grooming\n",
              "0              hit:0_conv:1              1  ...        0        0\n",
              "1              hit:0_conv:1              2  ...        0        0\n",
              "2              hit:0_conv:1              3  ...        0        0\n",
              "3              hit:0_conv:1              4  ...        0        0\n",
              "4              hit:0_conv:1              5  ...        0        0\n",
              "...                     ...            ...  ...      ...      ...\n",
              "76663  hit:12424_conv:24848              5  ...        0        0\n",
              "76664  hit:12424_conv:24849              1  ...        0        0\n",
              "76665  hit:12424_conv:24849              2  ...        0        0\n",
              "76666  hit:12424_conv:24849              3  ...        0        0\n",
              "76667  hit:12424_conv:24849              4  ...        0        0\n",
              "\n",
              "[76668 rows x 93 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR4HI3VSwdIJ"
      },
      "source": [
        "##Feature Builder Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "545JwQedwe-a"
      },
      "source": [
        "class Sequencer(object):\n",
        "    def __init__(self, tokens, bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>'):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "\n",
        "        self.pad_index = self.add_token(pad_token)\n",
        "        self.unk_index = self.add_token(unk_token) \n",
        "        self.bos_index = self.add_token(bos_token)\n",
        "        self.eos_index = self.add_token(eos_token)\n",
        "\n",
        "        for token in tokens:\n",
        "            self.add_token(token)\n",
        "\n",
        "    def add_token(self, token):\n",
        "\n",
        "        self.word2idx[token] = new_index = len(self.word2idx)\n",
        "        self.idx2word[new_index] = token\n",
        "\n",
        "        return new_index\n",
        "\n",
        "    def encode(self, tokens):\n",
        "        # Input will look like:\n",
        "        # [<s>, w1, w2, ..., wn, </s>]\n",
        "\n",
        "        sequence = [self.bos_index]\n",
        "        for token in tokens:\n",
        "\n",
        "            index = self.word2idx.get(token, self.unk_index)\n",
        "            sequence.append(index)\n",
        "        sequence.append(self.eos_index)\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def create_padded_tensor(self, sequences):\n",
        "        # Input: [[4, 2, 3], [5, 4, 2, 4, 6]]\n",
        "        # Output:\n",
        "        # Tensor\n",
        "        # 4 2 3 0 0\n",
        "        # 5 4 2 4 6\n",
        "\n",
        "        # Given a list of sequences, pad all to the same length\n",
        "        lengths = [len(sequence) for sequence in sequences]\n",
        "        max_seq_len = max(lengths)\n",
        "        tensor = torch.full((len(sequences), max_seq_len), self.pad_index, dtype=torch.long)\n",
        "\n",
        "        for i, sequence in enumerate(sequences):\n",
        "            for j, token in enumerate(sequence):\n",
        "                tensor[i][j] = token\n",
        "        return tensor\n",
        "\n",
        "# For converting labels into indices\n",
        "class LabelIndexer(object):\n",
        "    def __init__(self, labels):\n",
        "        self.label2idx = {label: i for i, label in enumerate(labels)}\n",
        "        self.idx2label = {i:label for label, i in self.label2idx.items()}\n",
        "        self.labels = labels\n",
        "\n",
        "    def encode(self, y):\n",
        "        return self.label2idx[y]\n",
        "\n",
        "    def encode_batch(self, ys):\n",
        "        return torch.LongTensor([self.encode(y) for y in ys])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMi7614Zwf7O"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4juajTNCxYlm"
      },
      "source": [
        "class EmpatheticDataset(Dataset):\n",
        "    def __init__(self, texts, liwc, labels, input_transformer, output_transformer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.liwc  = liwc.values\n",
        "        self.input_transformer = input_transformer\n",
        "        self.output_transformer = output_transformer\n",
        "\n",
        "    def __getitem__(self, index): # Return a single example\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "        liwc = self.liwc[index]\n",
        "        x_liwc = torch.tensor(liwc)\n",
        "        x = self.input_transformer(text)\n",
        "        y = self.output_transformer(label)       \n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2zZqanbdeiK"
      },
      "source": [
        "class EmpatheticDatasetLIWC(Dataset):\r\n",
        "    def __init__(self, texts, liwc, labels, input_transformer, output_transformer):\r\n",
        "        self.texts = texts\r\n",
        "        self.labels = labels\r\n",
        "        self.liwc  = liwc.values\r\n",
        "        self.input_transformer = input_transformer\r\n",
        "        self.output_transformer = output_transformer\r\n",
        "\r\n",
        "    def __getitem__(self, index): # Return a single example\r\n",
        "        text = self.texts[index]\r\n",
        "        label = self.labels[index]\r\n",
        "        liwc = self.liwc[index]\r\n",
        "        x_liwc = torch.tensor(liwc)\r\n",
        "        x = self.input_transformer(text)\r\n",
        "        y = self.output_transformer(label)       \r\n",
        "        return x_liwc.float(), y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb728zcXx2Nd"
      },
      "source": [
        "## Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iTOIdN2x5vv"
      },
      "source": [
        "class MultiClassTrainer(object):\n",
        "    \"\"\"\n",
        "    Trainer for training a multi-class classification model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, device=\"cpu\", log_every_n=None):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        self.log_every_n = log_every_n if log_every_n else 0\n",
        "\n",
        "\n",
        "    def _print_summary(self):\n",
        "        print(self.model)\n",
        "        print(self.optimizer)\n",
        "        print(self.loss_fn)\n",
        "\n",
        "    def train(self, loader):\n",
        "        \"\"\"\n",
        "        Run a single epoch of training\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.train() # Run model in training mode\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        for i, batch in tqdm(enumerate(loader)):\n",
        "            batch_size = batch[0].shape[0]\n",
        "            self.optimizer.zero_grad() # Always set gradient to 0 before computing it\n",
        "\n",
        "            logits = self.model(batch[0].to(self.device)) # Forward pass, # Wx + b\n",
        "            #print(logits)\n",
        "            #print(batch[1].view(-1).to(self.device))\n",
        "            loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss: Cross entropy loss\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            \n",
        "\n",
        "            running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "\n",
        "            if self.log_every_n and i % self.log_every_n == 0:\n",
        "                print(\"Running loss: \", running_loss)\n",
        "\n",
        "            running_loss_history.append(running_loss)\n",
        "\n",
        "            loss.backward() # Perform backprop, which will compute dL/dw\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 3.0)\n",
        "            self.optimizer.step() # Update step: w = w - eta * dL / dW\n",
        "\n",
        "        print(\"Epoch completed!\")\n",
        "        print(\"Epoch Loss: \", running_loss)\n",
        "        print(\"Epoch Perplexity: \", math.exp(running_loss))\n",
        "\n",
        "        # The history information can allow us to draw a loss plot\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def evaluate(self, loader, labels):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a validation set\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval() # Run model in eval mode (disables dropout layer)\n",
        "\n",
        "        batch_wise_true_labels = []\n",
        "        batch_wise_predictions = []\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient computation - required only during training\n",
        "            for i, batch in tqdm(enumerate(loader)):\n",
        "                # batch[0] shape: (batch_size, input_size)\n",
        "\n",
        "                logits = self.model(batch[0].to(self.device)) # Run forward pass (except we don't store gradients)\n",
        "                # logits shape: (batch_size, num_classes)\n",
        "                \n",
        "                loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss\n",
        "                # No backprop is done during validation\n",
        "                loss_history.append(loss.item())\n",
        "\n",
        "                running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "                \n",
        "                running_loss_history.append(running_loss)\n",
        "\n",
        "                # Converts the raw outputs into probabilities for each class using softmax\n",
        "                probs = F.softmax(logits, dim=-1) \n",
        "                # probs shape: (batch_size, num_classes)\n",
        "\n",
        "                predictions = torch.argmax(probs, dim=-1) # Output predictions\n",
        "                # predictions shape: (batch_size)\n",
        "\n",
        "                batch_wise_true_labels.append(batch[1].tolist())\n",
        "                batch_wise_predictions.append(predictions.tolist())\n",
        "        \n",
        "        # flatten the list of predictions using itertools\n",
        "        all_true_labels = list(chain.from_iterable(batch_wise_true_labels))\n",
        "        all_predictions = list(chain.from_iterable(batch_wise_predictions))\n",
        "\n",
        "        # Now we can generate a classification report\n",
        "        print(\"Classification report after epoch:\")\n",
        "        print(classification_report(all_true_labels, all_predictions))\n",
        "        print(confusion_matrix(all_true_labels,all_predictions))\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def get_model_dict(self):\n",
        "        return self.model.state_dict()\n",
        "\n",
        "    def run_training(self, train_loader, valid_loader, labels, n_epochs=10):\n",
        "        # Useful for us to review what experiment we're running\n",
        "        # Normally, you'd want to save this to a file\n",
        "        self._print_summary()\n",
        "\n",
        "        train_losses = []\n",
        "        train_running_losses = []\n",
        "\n",
        "        valid_losses = []\n",
        "        valid_running_losses = []\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            loss_history, running_loss_history = self.train(train_loader)\n",
        "            valid_loss_history, valid_running_loss_history = self.evaluate(valid_loader, labels)\n",
        "\n",
        "            train_losses.append(loss_history)\n",
        "            train_running_losses.append(running_loss_history)\n",
        "\n",
        "            valid_losses.append(valid_loss_history)\n",
        "            valid_running_losses.append(valid_running_loss_history)\n",
        "\n",
        "        # Training done, let's look at the loss curves\n",
        "        all_train_losses = list(chain.from_iterable(train_losses))\n",
        "        all_train_running_losses = list(chain.from_iterable(train_running_losses))\n",
        "\n",
        "        all_valid_losses = list(chain.from_iterable(valid_losses))\n",
        "        all_valid_running_losses = list(chain.from_iterable(valid_running_losses))\n",
        "\n",
        "        train_epoch_idx = range(len(all_train_losses))\n",
        "        valid_epoch_idx = range(len(all_valid_losses))\n",
        "        # sns.lineplot(epoch_idx, all_losses)\n",
        "        sns.lineplot(x=train_epoch_idx, y=all_train_running_losses)\n",
        "        sns.lineplot(x=valid_epoch_idx, y=all_valid_running_losses)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ApvqMmyZf9"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5S8ZV6hzquO"
      },
      "source": [
        "def convert_column(dataset, column): \n",
        "   texts = [text for text in dataset[column]]\n",
        "   texts = (*texts,)\n",
        "   return texts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f5V8vsj9Vbd"
      },
      "source": [
        "listener_train = train.loc[train[\"speaker_label\"] == \"listener\"]\r\n",
        "speaker_train = train.loc[train[\"speaker_label\"] == \"speaker\"]\r\n",
        "\r\n",
        "listener_valid = valid.loc[valid[\"speaker_label\"] == \"listener\"]\r\n",
        "speaker_valid = valid.loc[valid[\"speaker_label\"] == \"speaker\"]\r\n",
        "\r\n",
        "listener_test = test.loc[train[\"speaker_label\"] == \"listener\"]\r\n",
        "speaker_test = test.loc[train[\"speaker_label\"] == \"speaker\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o33KNEGzjzy"
      },
      "source": [
        "train_liwc = train[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "valid_liwc = valid[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "test_liwc = test[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "\n",
        "\n",
        "# prompts\n",
        "train_prompt = convert_column(train,'clean_prompt')\n",
        "valid_prompt = convert_column(valid,'clean_prompt')\n",
        "test_prompt = convert_column(test,'clean_prompt')\n",
        "\n",
        "# utterances\n",
        "train_utterance = convert_column(train,'clean_utterance')\n",
        "valid_utterance = convert_column(valid,'clean_utterance')\n",
        "test_utterance = convert_column(test,'clean_utterance')\n",
        "\n",
        "# 32 emotions\n",
        "train_context_labels = convert_column(train,'context')\n",
        "valid_context_labels = convert_column(valid,'context')\n",
        "test_context_labels = convert_column(test,'context')\n",
        "\n",
        "# 3 emotions\n",
        "train_grouped_labels = convert_column(train,'emotion_category')\n",
        "valid_grouped_labels = convert_column(valid, \"emotion_category\")\n",
        "test_grouped_labels = convert_column(test, \"emotion_category\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VyDi84tiuyj"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cuSq_TMSjm0"
      },
      "source": [
        "#Only run when testing on PROMPTS\n",
        "# Removing duplicates\n",
        "train = train.drop_duplicates(subset=['clean_prompt'])\n",
        "valid = valid.drop_duplicates(subset=['clean_prompt'])\n",
        "#test = test.drop_duplicates(subset=['clean_prompt'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkdgomE0zAZ4"
      },
      "source": [
        "def flatten_list(labels):\n",
        "  return list(set([tag for tag_seq in labels for tag in tag_seq.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OVXsoxfz0iF"
      },
      "source": [
        "train_prompt_flattened = flatten_list(train_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbBEppacbNEY"
      },
      "source": [
        "# Prompts Dataloader w/ 32 labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQo6N-YTlgRP"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_context_labels+valid_context_labels)))\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\n",
        "\n",
        "# embeddings\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\n",
        "\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\n",
        "\n",
        "def prepare_batch(batch, sequencer):\n",
        "    # batch: [batch_len, (text, label)]\n",
        "    texts, labels = zip(*batch)\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\n",
        "    return (text_tensor, torch.stack(labels))\n",
        "\n",
        "\n",
        "# wrapper class without liwc features\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\n",
        "\n",
        "# wrapper class with liwc features\n",
        "train_sequence_dataset = EmpatheticDataset(train_prompt, train_liwc, train_context_labels, sequence_input_transformer_, output_transformer)\n",
        "valid_sequence_dataset = EmpatheticDataset(valid_prompt, valid_liwc,  valid_context_labels, sequence_input_transformer_, output_transformer)\n",
        "test_sequence_dataset = EmpatheticDataset(test_prompt, test_liwc, test_context_labels, sequence_input_transformer_, output_transformer)\n",
        "\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=64,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmKD503pbaLQ"
      },
      "source": [
        "# Utterance Dataloader w/ 32 labels\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQb444DpbZiy"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_context_labels+valid_context_labels)))\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\n",
        "\n",
        "# embeddings\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\n",
        "\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\n",
        "\n",
        "def prepare_batch(batch, sequencer):\n",
        "    # batch: [batch_len, (text, label)]\n",
        "    texts, labels = zip(*batch)\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\n",
        "    return (text_tensor, torch.stack(labels))\n",
        "\n",
        "\n",
        "# wrapper class without liwc features\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\n",
        "\n",
        "# wrapper class with liwc features\n",
        "train_sequence_dataset = EmpatheticDataset(train_prompt, train_liwc, train_context_labels, sequence_input_transformer_, output_transformer)\n",
        "valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_liwc,  valid_context_labels, sequence_input_transformer_, output_transformer)\n",
        "test_sequence_dataset = EmpatheticDataset(test_utterance, test_liwc, test_context_labels, sequence_input_transformer_, output_transformer)\n",
        "\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=5, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=5,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJtTUgldMAO"
      },
      "source": [
        "# LIWC Dataloader w/ 32 labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD6o-tyzd7BE"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_context_labels+valid_context_labels)))\r\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\r\n",
        "\r\n",
        "# embeddings\r\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\r\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\r\n",
        "\r\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\r\n",
        "\r\n",
        "def prepare_batch(batch, sequencer):\r\n",
        "    # batch: [batch_len, (text, label)]\r\n",
        "    texts, labels = zip(*batch)\r\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\r\n",
        "    return (text_tensor, torch.stack(labels))\r\n",
        "\r\n",
        "\r\n",
        "# wrapper class without liwc features\r\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\r\n",
        "\r\n",
        "# wrapper class with liwc features\r\n",
        "train_sequence_dataset = EmpatheticDatasetLIWC(train_prompt, train_liwc, train_context_labels, sequence_input_transformer_, output_transformer)\r\n",
        "valid_sequence_dataset = EmpatheticDatasetLIWC(valid_prompt, valid_liwc,  valid_context_labels, sequence_input_transformer_, output_transformer)\r\n",
        "test_sequence_dataset = EmpatheticDatasetLIWC(test_prompt, test_liwc, test_context_labels, sequence_input_transformer_, output_transformer)\r\n",
        "\r\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=16, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=16, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=16,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU4-LPelxufo"
      },
      "source": [
        "class TextConvolver(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, kernel_sizes, channel_size=32, dropout=False, dropout_p=0.1, embedding_dim=128):\n",
        "        super(TextConvolver, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.add_dropout = dropout\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Define an iterable set of parallel layers which are given the same input\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, channel_size, kernel_size) for kernel_size in kernel_sizes])\n",
        "\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * channel_size, output_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x size: [batch_size, seq_len]\n",
        "\n",
        "        embed = self.embedding(x).transpose(1, 2)\n",
        "\n",
        "        convs = [F.relu(conv(embed)) for conv in self.convs]\n",
        "        # [num_filters, (batch_size, out_dim, seq_len)]\n",
        "\n",
        "        maxs = [F.max_pool1d(conv_out, conv_out.shape[2]).squeeze(2) for conv_out in convs] # Max pool across time\n",
        "        # After max pooling: [num_filters, (batch_size, channel_size, 1)]\n",
        "        # After squeezing: [num_filters, (batch_size, channel_size)]\n",
        "\n",
        "        flattened_maxs = torch.cat(maxs, dim=1)\n",
        "        # [batch_size, num_filters * channel_size]\n",
        "\n",
        "\n",
        "        # logits shape: [batch_size, output_size]\n",
        "        logits = self.fc(self.dropout(flattened_maxs))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_IpI1F9S5Vz"
      },
      "source": [
        "# To print entire confusion matrix\n",
        "import sys\n",
        "import numpy\n",
        "numpy.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odSIWNS-nk30",
        "outputId": "3383be50-312a-4014-bcc2-0362f85135a1"
      },
      "source": [
        "vocab_size =  len(train_liwc.columns)\r\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e2d8db6b26a24888afcbd24df6e51857",
            "ab8571bfab8f42909115fa57d4ef3ef9",
            "91d7bd87cffa4ce0806c74def73d246d",
            "8e5ff34fe89e433ba417b02a085e7f00",
            "9cd3eafb08fc4475889b89e160197776",
            "23c4abf2e66e494ba6efbac7fd1a5ac5",
            "2e30742559c645e9a7081f76aeda168a",
            "24e7f0db0d0344bda3cbdc48ee5213ca"
          ]
        },
        "id": "VlPiFxkIliPa",
        "outputId": "da02b74e-15e3-4326-8099-cbd57e7ce0fa"
      },
      "source": [
        "vocab_size = len(sequencer_.idx2word)\n",
        "print(vocab_size)\n",
        "output_size = len(label_indexer.idx2label)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "cnn = TextConvolver(vocab_size, output_size, [3, 4, 5], channel_size=100, dropout_p=0.5)\n",
        "cnn_optimizer = optim.Adam(cnn.parameters(), lr=1e-3)\n",
        "\n",
        "cnn_trainer = MultiClassTrainer(cnn, cnn_optimizer, loss_fn, device=device, log_every_n=5)\n",
        "cnn_trainer.run_training(train_sequence_loader, valid_sequence_loader, label_indexer.labels, n_epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14889\n",
            "TextConvolver(\n",
            "  (embedding): Embedding(14889, 128)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(128, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(128, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(128, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (fc): Linear(in_features=300, out_features=32, bias=True)\n",
            ")\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "CrossEntropyLoss()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2d8db6b26a24888afcbd24df6e51857",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss:  3.692556381225586\n",
            "Running loss:  3.808782617251078\n",
            "Running loss:  3.7532089840282095\n",
            "Running loss:  3.7761488556861877\n",
            "Running loss:  3.795727797916957\n",
            "Running loss:  3.780222470943744\n",
            "Running loss:  3.7705517353550078\n",
            "Running loss:  3.7475896742608805\n",
            "Running loss:  3.7446881212839256\n",
            "Running loss:  3.7416143780169273\n",
            "Running loss:  3.74168261827207\n",
            "Running loss:  3.7336221975939607\n",
            "Running loss:  3.7234156718019573\n",
            "Running loss:  3.7168449344057017\n",
            "Running loss:  3.7112604228543558\n",
            "Running loss:  3.7029821057068673\n",
            "Running loss:  3.698647943543799\n",
            "Running loss:  3.6922483776890953\n",
            "Running loss:  3.687608506653335\n",
            "Running loss:  3.682280677060286\n",
            "Running loss:  3.6874932605441257\n",
            "Running loss:  3.685778485154206\n",
            "Running loss:  3.6828074884844257\n",
            "Running loss:  3.6788689000853174\n",
            "Running loss:  3.6754151750202015\n",
            "Running loss:  3.671504713240123\n",
            "Running loss:  3.669639216124556\n",
            "Running loss:  3.666111472774954\n",
            "Running loss:  3.667567192239964\n",
            "Running loss:  3.661775709831551\n",
            "Running loss:  3.6613040980913776\n",
            "Running loss:  3.66116063717084\n",
            "Running loss:  3.6597607076538266\n",
            "Running loss:  3.658636536942906\n",
            "Running loss:  3.6547900863558223\n",
            "Running loss:  3.657598621465942\n",
            "Running loss:  3.653913987934259\n",
            "Running loss:  3.6524984746850935\n",
            "Running loss:  3.6512982732962547\n",
            "Running loss:  3.6480937210880966\n",
            "Running loss:  3.6437221854480333\n",
            "Running loss:  3.641636266291719\n",
            "Running loss:  3.640061029325728\n",
            "Running loss:  3.635049355250816\n",
            "Running loss:  3.6331698905288885\n",
            "Running loss:  3.6312277960566277\n",
            "Running loss:  3.6273512778344075\n",
            "Running loss:  3.627166027739895\n",
            "Running loss:  3.6227330459104023\n",
            "Running loss:  3.6238365628855\n",
            "Running loss:  3.6232913746776787\n",
            "Running loss:  3.621333325281737\n",
            "Running loss:  3.6203598884787116\n",
            "Running loss:  3.617481240652555\n",
            "Running loss:  3.615892972453491\n",
            "Running loss:  3.614677450795102\n",
            "Running loss:  3.6134923021988485\n",
            "Running loss:  3.61381687567784\n",
            "Running loss:  3.610936464722621\n",
            "Running loss:  3.61035904127198\n",
            "Running loss:  3.607637709557413\n",
            "Running loss:  3.6060849615171815\n",
            "Running loss:  3.6076432217355685\n",
            "Running loss:  3.6055390140678285\n",
            "Running loss:  3.6034370761051333\n",
            "Running loss:  3.60458774288739\n",
            "Running loss:  3.6034578189388844\n",
            "Running loss:  3.6020479542868458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-78b433283c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcnn_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiClassTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_every_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcnn_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequence_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sequence_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_indexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-cc610ecad0d3>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(self, train_loader, valid_loader, labels, n_epochs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mvalid_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_running_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-cc610ecad0d3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mrunning_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Always set gradient to 0 before computing it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-2fa4d26ede9d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtest_sequence_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmpatheticDatasetLIWC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_liwc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_context_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_input_transformer_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_transformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain_sequence_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequence_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequencer_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mvalid_sequence_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_sequence_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequencer_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtest_sequence_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequence_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequencer_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-2fa4d26ede9d>\u001b[0m in \u001b[0;36mprepare_batch\u001b[0;34m(batch, sequencer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# batch: [batch_len, (text, label)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtext_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequencer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_padded_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e7a2ec4b2e40>\u001b[0m in \u001b[0;36mcreate_padded_tensor\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9bacbac970e2488a859a3e49fffcf8cc",
            "e0db092d72384b7fa1c498757ef6bb36",
            "ac9db6ea7db94614b8fe81c64838c416",
            "365e5e0c725c42ed8b866917c248f3bc",
            "fa6420fa26434237b85f3b4a27844fea",
            "e0855ea2c8334a18b9111efaa4ce9ea1",
            "2d320df5c7214b24b6f190b9c8a66bc5",
            "1d306ebba9a0435cb6676fcfefbea0f7",
            "964ec6642dbc4b09bae8ac32f3054ca5",
            "c6990569673d4d74aab580555a077a0f",
            "eb93278986be42008a063cdaba5bb778",
            "aa6dbfadbcd14455b9ec222243ecfcb5",
            "1ee50b78245f4acaaafab8a675459fbe",
            "7f55cd36d2e644abb92ee5a73a32cb03",
            "00e49a45784e41aa88d0899016989624",
            "798fc14f65fa41ba9080d117bc42aa19"
          ]
        },
        "id": "wJuE4H9214DL",
        "outputId": "15091dca-0fab-405c-cd83-c3751f63d35a"
      },
      "source": [
        "cnn_trainer.run_training(train_sequence_loader, test_sequence_loader, label_indexer.labels, n_epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextConvolver(\n",
            "  (embedding): Embedding(14889, 128)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(128, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(128, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(128, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
            ")\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "CrossEntropyLoss()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bacbac970e2488a859a3e49fffcf8cc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss:  0.8703696131706238\n",
            "Running loss:  0.8648138642311096\n",
            "Running loss:  0.9585357470945879\n",
            "Running loss:  0.9753799475729467\n",
            "Running loss:  0.9571614804722015\n",
            "Running loss:  0.9567580475256994\n",
            "Running loss:  0.9498927035639364\n",
            "Running loss:  0.9348151551352607\n",
            "Running loss:  0.9347373407061507\n",
            "Running loss:  0.9338794920755469\n",
            "Running loss:  0.9367607200846952\n",
            "Running loss:  0.9362953518118176\n",
            "Running loss:  0.9358422746423813\n",
            "Running loss:  0.9463018440839015\n",
            "Running loss:  0.9529291500507945\n",
            "Running loss:  0.9457720376943286\n",
            "Running loss:  0.9395514461729261\n",
            "Running loss:  0.9374395685140475\n",
            "Running loss:  0.9329804103453079\n",
            "Running loss:  0.9224773428092399\n",
            "Running loss:  0.9264996577017377\n",
            "Running loss:  0.9233487608297816\n",
            "Running loss:  0.9274734451964095\n",
            "Running loss:  0.9206180017569968\n",
            "Running loss:  0.9208547142911546\n",
            "Running loss:  0.9248324485998303\n",
            "Running loss:  0.9251671287849657\n",
            "Running loss:  0.9289937518975311\n",
            "Running loss:  0.9283382169743798\n",
            "Running loss:  0.9345917754793818\n",
            "Running loss:  0.9297520684090669\n",
            "Running loss:  0.9314883561470567\n",
            "Running loss:  0.9310138573557692\n",
            "Running loss:  0.926914922444217\n",
            "Running loss:  0.9235720097670079\n",
            "Running loss:  0.9229278923435642\n",
            "Running loss:  0.9265372499576587\n",
            "Running loss:  0.9281117800743348\n",
            "Running loss:  0.9295265871192772\n",
            "Running loss:  0.9312853764514534\n",
            "Running loss:  0.9294467064278635\n",
            "Running loss:  0.9292384956068205\n",
            "Running loss:  0.9299771249011795\n",
            "Running loss:  0.9308137523907203\n",
            "Running loss:  0.930292009228495\n",
            "Running loss:  0.9291539727586561\n",
            "Running loss:  0.9279393187332978\n",
            "Running loss:  0.9300089534056388\n",
            "Running loss:  0.9296905069430339\n",
            "Running loss:  0.9273974774329641\n",
            "Running loss:  0.929132681918809\n",
            "Running loss:  0.9335455622058361\n",
            "Running loss:  0.9365058095975852\n",
            "Running loss:  0.9365796803083634\n",
            "Running loss:  0.9361273404417002\n",
            "Running loss:  0.9360735094633653\n",
            "Running loss:  0.9354842960622384\n",
            "Running loss:  0.9331701016509446\n",
            "Running loss:  0.933717671743373\n",
            "Running loss:  0.9327466330818225\n",
            "Running loss:  0.9323282388357623\n",
            "Running loss:  0.9335042002543903\n",
            "Running loss:  0.9347636550185763\n",
            "Running loss:  0.9349897594391544\n",
            "Running loss:  0.9339073229801615\n",
            "Running loss:  0.9330207899304254\n",
            "Running loss:  0.9333011664652751\n",
            "Running loss:  0.9332768631478149\n",
            "Running loss:  0.9337200683232975\n",
            "Running loss:  0.93319953452645\n",
            "Running loss:  0.9330867415140157\n",
            "Running loss:  0.9350130917985786\n",
            "Running loss:  0.9352906980342812\n",
            "Running loss:  0.9335645310214308\n",
            "Running loss:  0.9327561023100366\n",
            "Running loss:  0.9324850680980274\n",
            "Running loss:  0.9310564120297667\n",
            "Running loss:  0.9311032190224047\n",
            "Running loss:  0.9311253170832952\n",
            "Running loss:  0.9317800628416465\n",
            "Running loss:  0.9326446829294028\n",
            "Running loss:  0.9331755639590652\n",
            "Running loss:  0.9314596402094021\n",
            "Running loss:  0.9325877112837937\n",
            "Running loss:  0.9333336121112886\n",
            "Running loss:  0.9333338438065399\n",
            "Running loss:  0.9339821239082001\n",
            "Running loss:  0.9350343749884074\n",
            "Running loss:  0.9343045666104273\n",
            "Running loss:  0.9342748838155259\n",
            "Running loss:  0.9333915940409487\n",
            "Running loss:  0.9350171189820556\n",
            "Running loss:  0.93406720857041\n",
            "Running loss:  0.9341170136764838\n",
            "Running loss:  0.934581091702617\n",
            "Running loss:  0.934092955303793\n",
            "Running loss:  0.9331022980555177\n",
            "Running loss:  0.933795549619345\n",
            "Running loss:  0.9331993399958017\n",
            "Running loss:  0.9329084840513043\n",
            "Running loss:  0.932981346300738\n",
            "Running loss:  0.9334239304772478\n",
            "Running loss:  0.9335526180127361\n",
            "Running loss:  0.9335995809745418\n",
            "Running loss:  0.9337011234774011\n",
            "Running loss:  0.9341752388631435\n",
            "Running loss:  0.9344898369128195\n",
            "Running loss:  0.9329846857199027\n",
            "Running loss:  0.9323690246081395\n",
            "Running loss:  0.9322233373647205\n",
            "Running loss:  0.9335087785487163\n",
            "Running loss:  0.9336516862507344\n",
            "Running loss:  0.9332617900588293\n",
            "Running loss:  0.9339903819476755\n",
            "Running loss:  0.9331257882970015\n",
            "Running loss:  0.9328991715899768\n",
            "Running loss:  0.9334786080862687\n",
            "Running loss:  0.9334417113671932\n",
            "Running loss:  0.9327732981158993\n",
            "Running loss:  0.932312594864192\n",
            "Running loss:  0.9327123637207333\n",
            "Running loss:  0.9326378473944394\n",
            "Running loss:  0.9328226286143599\n",
            "Running loss:  0.9335892154024792\n",
            "Running loss:  0.9320085466195994\n",
            "Running loss:  0.9351449261267725\n",
            "Running loss:  0.9345051304285197\n",
            "Running loss:  0.9347566827288213\n",
            "Running loss:  0.9347311691635297\n",
            "Running loss:  0.9351273936568397\n",
            "Running loss:  0.9342090278726567\n",
            "Running loss:  0.934523529214103\n",
            "Running loss:  0.9345353977041776\n",
            "Running loss:  0.9334963266555967\n",
            "Running loss:  0.9335544216828264\n",
            "Running loss:  0.933630299815059\n",
            "Running loss:  0.9333630568305472\n",
            "Running loss:  0.9334387583739553\n",
            "Running loss:  0.933218385393471\n",
            "Running loss:  0.9330970834212741\n",
            "Running loss:  0.932835958993724\n",
            "Running loss:  0.9323164289632533\n",
            "Running loss:  0.9318013651461541\n",
            "Running loss:  0.9321340245907533\n",
            "Running loss:  0.93199623052687\n",
            "Running loss:  0.9319177955471123\n",
            "Running loss:  0.9325739732356144\n",
            "Running loss:  0.9334388823936814\n",
            "Running loss:  0.9342262280775627\n",
            "Running loss:  0.9343659371217518\n",
            "Running loss:  0.9335029400617241\n",
            "Running loss:  0.9328666012438516\n",
            "Running loss:  0.9331118523212988\n",
            "Running loss:  0.9334962553984191\n",
            "Running loss:  0.9340761490214504\n",
            "Running loss:  0.93434663877352\n",
            "Running loss:  0.9333895943992117\n",
            "Running loss:  0.9335779870739419\n",
            "Running loss:  0.9335575320777941\n",
            "Running loss:  0.9337445224200063\n",
            "Running loss:  0.9341122215606746\n",
            "Running loss:  0.934169129165763\n",
            "Running loss:  0.9337483624641758\n",
            "Running loss:  0.934347740721469\n",
            "Running loss:  0.9342850898273389\n",
            "Running loss:  0.9341786488638086\n",
            "Running loss:  0.9342172604892515\n",
            "Running loss:  0.9345823768793681\n",
            "Running loss:  0.9347351797979311\n",
            "Running loss:  0.9353165490548379\n",
            "Running loss:  0.9355409396100968\n",
            "Running loss:  0.9352790394397538\n",
            "Running loss:  0.9354926965931704\n",
            "Running loss:  0.9351512427693425\n",
            "Running loss:  0.934916768843216\n",
            "Running loss:  0.9348642148383675\n",
            "Running loss:  0.9347125587479616\n",
            "Running loss:  0.9351273284001489\n",
            "Running loss:  0.9348091186765334\n",
            "Running loss:  0.9354393080409084\n",
            "Running loss:  0.935570701113287\n",
            "Running loss:  0.9352754721578384\n",
            "Running loss:  0.9360916893908798\n",
            "Running loss:  0.9361189601853426\n",
            "Running loss:  0.9356091064557709\n",
            "Running loss:  0.9347362892931533\n",
            "Running loss:  0.934422169989085\n",
            "Running loss:  0.9342248544861108\n",
            "Running loss:  0.9343882167630697\n",
            "Running loss:  0.9339772321079044\n",
            "Running loss:  0.934205994934439\n",
            "Running loss:  0.9340129160482016\n",
            "Running loss:  0.9342600806818793\n",
            "Running loss:  0.9343894417735114\n",
            "Running loss:  0.9344465283851545\n",
            "Running loss:  0.934425680058413\n",
            "Running loss:  0.9344308338641636\n",
            "Running loss:  0.9341628741903675\n",
            "Running loss:  0.9339138078160293\n",
            "Running loss:  0.9339240867210682\n",
            "Running loss:  0.9332017406717049\n",
            "Running loss:  0.9337353691547577\n",
            "Running loss:  0.9335753296296511\n",
            "Running loss:  0.933210988448361\n",
            "Running loss:  0.933439786181978\n",
            "Running loss:  0.9332888412196737\n",
            "Running loss:  0.9325068024150619\n",
            "Running loss:  0.9329090893383656\n",
            "Running loss:  0.9336114658631467\n",
            "Running loss:  0.9334610616841695\n",
            "Running loss:  0.9336245898175086\n",
            "Running loss:  0.9333071666346362\n",
            "Running loss:  0.9330503675423072\n",
            "Running loss:  0.9328099532266054\n",
            "Running loss:  0.932542850824298\n",
            "Running loss:  0.9324221854205474\n",
            "Running loss:  0.9329102245003515\n",
            "Running loss:  0.9331340085625874\n",
            "Running loss:  0.9333830195539711\n",
            "Running loss:  0.9332334561287059\n",
            "Running loss:  0.9330636813465198\n",
            "Running loss:  0.9334210649968063\n",
            "Running loss:  0.9330718627285042\n",
            "Running loss:  0.9331233017547164\n",
            "Running loss:  0.9329700457741391\n",
            "Running loss:  0.9334592759397492\n",
            "Running loss:  0.9333448332563961\n",
            "Running loss:  0.9330684416730647\n",
            "Running loss:  0.9331782814915202\n",
            "Running loss:  0.9329352391327871\n",
            "Running loss:  0.9325005811882691\n",
            "Running loss:  0.9324518680366274\n",
            "Running loss:  0.9321806786596321\n",
            "Running loss:  0.9325580745679853\n",
            "Running loss:  0.9323715340062971\n",
            "Running loss:  0.9327226897486219\n",
            "Running loss:  0.9327035782500717\n",
            "Running loss:  0.9320175671356331\n",
            "Running loss:  0.931887362856109\n",
            "Running loss:  0.931598727719043\n",
            "Running loss:  0.9321127698681542\n",
            "Running loss:  0.9325606598585208\n",
            "Running loss:  0.9331022734783787\n",
            "Running loss:  0.9331701788561131\n",
            "Running loss:  0.933378338520885\n",
            "Running loss:  0.9333875010103044\n",
            "Running loss:  0.9332916398633319\n",
            "Running loss:  0.9328891239023526\n",
            "Running loss:  0.9335280887836614\n",
            "Running loss:  0.9334689773870326\n",
            "Running loss:  0.9337181213090754\n",
            "Running loss:  0.933602513686108\n",
            "Running loss:  0.934238919419585\n",
            "Running loss:  0.9345995438701551\n",
            "Running loss:  0.934246130246614\n",
            "Running loss:  0.9345782541182354\n",
            "Running loss:  0.9343768743497153\n",
            "Running loss:  0.9340583928636227\n",
            "Running loss:  0.9350066105124569\n",
            "Running loss:  0.9354213764086188\n",
            "Running loss:  0.9352120289337083\n",
            "Running loss:  0.9351432349926486\n",
            "Running loss:  0.9346933414879695\n",
            "Running loss:  0.9351864376510174\n",
            "Running loss:  0.9351302889781324\n",
            "Running loss:  0.9352887075501155\n",
            "Running loss:  0.9352942017753\n",
            "Running loss:  0.9355169396825186\n",
            "Running loss:  0.9354243694980324\n",
            "Running loss:  0.9353308057714189\n",
            "Running loss:  0.9351714811529793\n",
            "Running loss:  0.9354335721527237\n",
            "Running loss:  0.9353907478810415\n",
            "Running loss:  0.9350339781225694\n",
            "Running loss:  0.935135969665258\n",
            "Running loss:  0.9350458500516976\n",
            "Running loss:  0.9352568952956127\n",
            "Running loss:  0.9353977984508229\n",
            "Running loss:  0.9354875353837862\n",
            "Running loss:  0.9363119674360862\n",
            "Running loss:  0.9364515522137281\n",
            "Running loss:  0.9367256241706161\n",
            "Running loss:  0.9363644212209788\n",
            "Running loss:  0.936243424855047\n",
            "Running loss:  0.9363741847430215\n",
            "Running loss:  0.9368272952495088\n",
            "Running loss:  0.9368048231556735\n",
            "Running loss:  0.9368970572616402\n",
            "Running loss:  0.9371124063408437\n",
            "Running loss:  0.9371555538263233\n",
            "Running loss:  0.9369583647058888\n",
            "Running loss:  0.9370763794733933\n",
            "Running loss:  0.9371232790623852\n",
            "Running loss:  0.9376715652207566\n",
            "Running loss:  0.9372369229671995\n",
            "Running loss:  0.9375036200129895\n",
            "Running loss:  0.9370429663542239\n",
            "Running loss:  0.9370129862110842\n",
            "Running loss:  0.9373746930473533\n",
            "Running loss:  0.9375105968372712\n",
            "Running loss:  0.9379126522082004\n",
            "Running loss:  0.9374520166778316\n",
            "Running loss:  0.9377857744496526\n",
            "Running loss:  0.9381441293495631\n",
            "Running loss:  0.9384755328797265\n",
            "Running loss:  0.9387781377076171\n",
            "Running loss:  0.9389469959547446\n",
            "Running loss:  0.9389908372346938\n",
            "Running loss:  0.9390574087189983\n",
            "Running loss:  0.9390197912132298\n",
            "Running loss:  0.9388572764273697\n",
            "Running loss:  0.9389730558220715\n",
            "Running loss:  0.9391670111354392\n",
            "Running loss:  0.9388601042682342\n",
            "Running loss:  0.9390003150394209\n",
            "Running loss:  0.9392917620651619\n",
            "Running loss:  0.9388811623348915\n",
            "Running loss:  0.9385954996225668\n",
            "Running loss:  0.9387423121127453\n",
            "Running loss:  0.9393322625031751\n",
            "Running loss:  0.9391431581370914\n",
            "Running loss:  0.9389507021583333\n",
            "Running loss:  0.9389405905053458\n",
            "Running loss:  0.9382399077536451\n",
            "Running loss:  0.9382395247189376\n",
            "Running loss:  0.9382831538588657\n",
            "Running loss:  0.9383417416539832\n",
            "Running loss:  0.9387985685052387\n",
            "Running loss:  0.9391163534903081\n",
            "Running loss:  0.9389900217151879\n",
            "Running loss:  0.9389868420156696\n",
            "Running loss:  0.939117776886853\n",
            "Running loss:  0.9387911581835215\n",
            "Running loss:  0.9388818480387461\n",
            "Running loss:  0.938932519377099\n",
            "Running loss:  0.9388544799533273\n",
            "Running loss:  0.9391215460630918\n",
            "Running loss:  0.9388665487616377\n",
            "Running loss:  0.9385178998544474\n",
            "Running loss:  0.9386255936169967\n",
            "Running loss:  0.9387754648310663\n",
            "Running loss:  0.9389888824150411\n",
            "Running loss:  0.9390407985654379\n",
            "Running loss:  0.9394046187331356\n",
            "Running loss:  0.9393545004394712\n",
            "Running loss:  0.9394290996316143\n",
            "Running loss:  0.9393029520788482\n",
            "Running loss:  0.9397218564207661\n",
            "Running loss:  0.9397833144301873\n",
            "Running loss:  0.9393617478706044\n",
            "Running loss:  0.9395589911209259\n",
            "Running loss:  0.93986577678107\n",
            "Running loss:  0.9402411346202688\n",
            "Running loss:  0.9399470239371441\n",
            "Running loss:  0.9398095128833751\n",
            "Running loss:  0.9397448849033673\n",
            "Running loss:  0.9395153002749669\n",
            "Running loss:  0.939429733314013\n",
            "Running loss:  0.9393669321908431\n",
            "Running loss:  0.9397302119928381\n",
            "Running loss:  0.9398681669020781\n",
            "Running loss:  0.9397991437254564\n",
            "Running loss:  0.9394994895146219\n",
            "Running loss:  0.939448593307435\n",
            "Running loss:  0.9395479642382571\n",
            "Running loss:  0.939373649681255\n",
            "Running loss:  0.9392763395116742\n",
            "Running loss:  0.9395273769018705\n",
            "Running loss:  0.9395741316884973\n",
            "Running loss:  0.93948294320835\n",
            "Running loss:  0.9393701182707531\n",
            "Running loss:  0.939653118911746\n",
            "Running loss:  0.939562076045134\n",
            "Running loss:  0.939328775688256\n",
            "Running loss:  0.9394642598230155\n",
            "Running loss:  0.939593630654218\n",
            "Running loss:  0.9396264683940455\n",
            "Running loss:  0.9394898152300823\n",
            "Running loss:  0.9392990247084952\n",
            "Running loss:  0.9390111411862747\n",
            "Running loss:  0.9388780756287685\n",
            "Running loss:  0.938684728563396\n",
            "Running loss:  0.9385820638441041\n",
            "Running loss:  0.939055847536797\n",
            "Running loss:  0.9392042614903579\n",
            "Running loss:  0.9392224291217673\n",
            "Running loss:  0.9392583949895561\n",
            "Running loss:  0.9393231783462469\n",
            "Running loss:  0.9390372548565196\n",
            "Running loss:  0.9390757563547889\n",
            "Running loss:  0.938984711557582\n",
            "Running loss:  0.9390531909368287\n",
            "Running loss:  0.9389761019154275\n",
            "Running loss:  0.9387426024050832\n",
            "Running loss:  0.9388752076784294\n",
            "Running loss:  0.9387892655093184\n",
            "Running loss:  0.938833935221057\n",
            "Running loss:  0.9387587528574509\n",
            "Running loss:  0.9387311710597647\n",
            "Running loss:  0.9387593868679422\n",
            "Running loss:  0.9386260496682377\n",
            "Running loss:  0.9387834808702844\n",
            "Running loss:  0.938671960544966\n",
            "Running loss:  0.9384881637340032\n",
            "Running loss:  0.9387193300946999\n",
            "Running loss:  0.938975737837437\n",
            "Running loss:  0.9393737008570455\n",
            "Running loss:  0.9394011471149735\n",
            "Running loss:  0.9394131120018729\n",
            "Running loss:  0.9396053710524527\n",
            "Running loss:  0.9396205618287348\n",
            "Running loss:  0.9401852249345434\n",
            "Running loss:  0.9402556082162395\n",
            "Running loss:  0.9402477754224079\n",
            "Running loss:  0.9403994189605\n",
            "Running loss:  0.9406051049457586\n",
            "Running loss:  0.9406122408596048\n",
            "Running loss:  0.9405225783957967\n",
            "Running loss:  0.9404223273098048\n",
            "Running loss:  0.9404586171375894\n",
            "Running loss:  0.9404153870946174\n",
            "Running loss:  0.9404459264260305\n",
            "Running loss:  0.940334099748586\n",
            "Running loss:  0.9403707840266893\n",
            "Running loss:  0.9401584733268785\n",
            "Running loss:  0.9401542451004887\n",
            "Running loss:  0.9402841588019886\n",
            "Running loss:  0.9402035772521405\n",
            "Running loss:  0.9397682981589092\n",
            "Running loss:  0.9395512602147058\n",
            "Running loss:  0.9397478728225658\n",
            "Running loss:  0.9395249816614982\n",
            "Running loss:  0.9395983099716789\n",
            "Running loss:  0.9398139226469643\n",
            "Running loss:  0.9395061440590838\n",
            "Running loss:  0.9394141581073848\n",
            "Running loss:  0.9396428351607803\n",
            "Running loss:  0.9398529756429107\n",
            "Running loss:  0.939725030806989\n",
            "Running loss:  0.9399269938143231\n",
            "Running loss:  0.9402732613789723\n",
            "Running loss:  0.9406154402417704\n",
            "Running loss:  0.9408517191443501\n",
            "Running loss:  0.9407966545234956\n",
            "Running loss:  0.9407179855860237\n",
            "Running loss:  0.9408809361646339\n",
            "Running loss:  0.9409449476930718\n",
            "Running loss:  0.9410043811169002\n",
            "Running loss:  0.9408794155827283\n",
            "Running loss:  0.9410179678553048\n",
            "Running loss:  0.9411288016587039\n",
            "Running loss:  0.9410612431242541\n",
            "Running loss:  0.9408601116206157\n",
            "Running loss:  0.9407752862517438\n",
            "Running loss:  0.9404886909278681\n",
            "Running loss:  0.940511461327701\n",
            "Running loss:  0.9404509822980511\n",
            "Running loss:  0.9405295331542789\n",
            "Running loss:  0.9406184652703974\n",
            "Running loss:  0.9405055527643466\n",
            "Running loss:  0.9406260594965224\n",
            "Running loss:  0.9407687772126795\n",
            "Running loss:  0.9406922920242325\n",
            "Running loss:  0.9406365127349359\n",
            "Running loss:  0.9406889876585903\n",
            "Running loss:  0.9407868947023378\n",
            "Running loss:  0.9408478904875093\n",
            "Running loss:  0.9409788805634196\n",
            "Running loss:  0.9410189232577738\n",
            "Running loss:  0.9410289271662411\n",
            "Running loss:  0.9413167992737492\n",
            "Running loss:  0.9411897963902752\n",
            "Running loss:  0.9413323076737738\n",
            "Running loss:  0.94107943496156\n",
            "Running loss:  0.9410323083073925\n",
            "Running loss:  0.9409596560027468\n",
            "Running loss:  0.9410677647099986\n",
            "Running loss:  0.9412652761218738\n",
            "Running loss:  0.9413823070061454\n",
            "Running loss:  0.9415445276263961\n",
            "Running loss:  0.9415324959145247\n",
            "Running loss:  0.9415361083802443\n",
            "Running loss:  0.9412887452401253\n",
            "Running loss:  0.9410411734504024\n",
            "Running loss:  0.9408679186239169\n",
            "Running loss:  0.9410813543930198\n",
            "Running loss:  0.9410252010121067\n",
            "Running loss:  0.9410583321586229\n",
            "Running loss:  0.9410495472364572\n",
            "Running loss:  0.9410068032835123\n",
            "Running loss:  0.9406553037990509\n",
            "Running loss:  0.9406369710978161\n",
            "Running loss:  0.9408758191353229\n",
            "Running loss:  0.9408042333228556\n",
            "Running loss:  0.9410288481035011\n",
            "Running loss:  0.9410479175880774\n",
            "Running loss:  0.9410266783246112\n",
            "Running loss:  0.9411598067799601\n",
            "Running loss:  0.941110567604381\n",
            "Running loss:  0.94104249135424\n",
            "Running loss:  0.9410528056576752\n",
            "Running loss:  0.9412042508816021\n",
            "Running loss:  0.9414667838220817\n",
            "Running loss:  0.9416887872691\n",
            "Running loss:  0.9415481126483406\n",
            "Running loss:  0.9413127881944514\n",
            "Running loss:  0.9413509231600921\n",
            "Running loss:  0.9414985746333279\n",
            "Running loss:  0.9413968896546829\n",
            "Running loss:  0.9416052094614611\n",
            "Running loss:  0.9413641241661292\n",
            "Running loss:  0.9415243168671927\n",
            "Running loss:  0.9417440020353803\n",
            "Running loss:  0.9417515740158593\n",
            "Running loss:  0.9415724722850377\n",
            "Running loss:  0.941572290122139\n",
            "Running loss:  0.9415155004642124\n",
            "Running loss:  0.9415661133451236\n",
            "Running loss:  0.9416345426908382\n",
            "Running loss:  0.9416790027142675\n",
            "Running loss:  0.9415507758181633\n",
            "Running loss:  0.9414035525810511\n",
            "Running loss:  0.9413753776018854\n",
            "Running loss:  0.9414731924413545\n",
            "Running loss:  0.9414945798082148\n",
            "Running loss:  0.9415146533487779\n",
            "Running loss:  0.9415438176787253\n",
            "Running loss:  0.9416203163055444\n",
            "Running loss:  0.941734633180329\n",
            "Running loss:  0.9416113554620422\n",
            "Running loss:  0.9413716176553569\n",
            "Running loss:  0.941274643415608\n",
            "Running loss:  0.94097527121297\n",
            "Running loss:  0.9408571467709028\n",
            "Running loss:  0.9408365824569799\n",
            "Running loss:  0.9406267517261624\n",
            "Running loss:  0.9406528148976608\n",
            "Running loss:  0.9405964852400179\n",
            "Running loss:  0.9404120774616416\n",
            "Running loss:  0.9406865373784433\n",
            "Running loss:  0.9406952908726726\n",
            "Running loss:  0.9406565783293448\n",
            "Running loss:  0.9404844604968502\n",
            "Running loss:  0.9406451597173371\n",
            "Running loss:  0.9408880876119159\n",
            "Running loss:  0.9409217125382924\n",
            "Running loss:  0.9407608475671159\n",
            "Running loss:  0.9408627344807344\n",
            "Running loss:  0.9408898580165648\n",
            "Running loss:  0.9411248991459896\n",
            "Running loss:  0.9412749194743552\n",
            "Running loss:  0.9413030757224098\n",
            "Running loss:  0.9414339053246567\n",
            "Running loss:  0.9416676879532516\n",
            "Running loss:  0.9415473642223627\n",
            "Running loss:  0.941606523753587\n",
            "Running loss:  0.9415629443741671\n",
            "Running loss:  0.9418031945343254\n",
            "Running loss:  0.9416809261635362\n",
            "Running loss:  0.9416311286677623\n",
            "Running loss:  0.941524825003351\n",
            "Running loss:  0.9415685231993224\n",
            "Running loss:  0.9415140346418878\n",
            "Running loss:  0.9414053511678837\n",
            "Running loss:  0.9412186110074622\n",
            "Running loss:  0.9416100707825247\n",
            "Running loss:  0.9416110948680866\n",
            "Running loss:  0.9414376688566801\n",
            "Running loss:  0.9412354013736046\n",
            "Running loss:  0.9413531559374834\n",
            "Running loss:  0.941250666218112\n",
            "Running loss:  0.9410187964763298\n",
            "Running loss:  0.9409274886805297\n",
            "Running loss:  0.9411398311719479\n",
            "Running loss:  0.9415466749306886\n",
            "Running loss:  0.9414821570746927\n",
            "Running loss:  0.9414894505326681\n",
            "Running loss:  0.9415010694034883\n",
            "Running loss:  0.9413909441210008\n",
            "Running loss:  0.9413623338904834\n",
            "Running loss:  0.941430325591947\n",
            "Running loss:  0.9413871208881401\n",
            "Running loss:  0.9412453412108719\n",
            "Running loss:  0.9413133368652385\n",
            "Running loss:  0.941407039348331\n",
            "Running loss:  0.9413166971379414\n",
            "Running loss:  0.9414801779195178\n",
            "Running loss:  0.9414503316138686\n",
            "Running loss:  0.941706763567142\n",
            "Running loss:  0.9418038597781894\n",
            "Running loss:  0.9417170841626827\n",
            "Running loss:  0.9418198243086013\n",
            "Running loss:  0.9419387569301888\n",
            "Running loss:  0.9420481250041743\n",
            "Running loss:  0.9420248992071543\n",
            "Running loss:  0.9419280388342446\n",
            "Running loss:  0.9420377612113963\n",
            "Running loss:  0.9421011287936102\n",
            "Running loss:  0.9420953950966463\n",
            "Running loss:  0.9419768949813941\n",
            "Running loss:  0.9418456808244028\n",
            "Running loss:  0.9418878592377264\n",
            "Running loss:  0.9419427991150949\n",
            "Running loss:  0.942132233861904\n",
            "Running loss:  0.9420384339487591\n",
            "Running loss:  0.9419973793994185\n",
            "Running loss:  0.9420080444711314\n",
            "Running loss:  0.9420254124286787\n",
            "Running loss:  0.9421716662113397\n",
            "Running loss:  0.941953762827622\n",
            "Running loss:  0.9420527974647301\n",
            "Running loss:  0.941810141800275\n",
            "Running loss:  0.9415453506894702\n",
            "Running loss:  0.9417914470581173\n",
            "Running loss:  0.9417304852349653\n",
            "Running loss:  0.9417203232665379\n",
            "Running loss:  0.941878956736238\n",
            "Running loss:  0.942018168569771\n",
            "Running loss:  0.9420468087182476\n",
            "Running loss:  0.9421766212436281\n",
            "Running loss:  0.9419310023153427\n",
            "Running loss:  0.9418858333280308\n",
            "Running loss:  0.9419596163224893\n",
            "Running loss:  0.9420561270742892\n",
            "Running loss:  0.9418110617152099\n",
            "Running loss:  0.9420505195036207\n",
            "Running loss:  0.942115772592874\n",
            "Running loss:  0.9420998678827783\n",
            "Running loss:  0.9422216301357557\n",
            "Running loss:  0.9422493435286153\n",
            "Running loss:  0.9419731502871947\n",
            "Running loss:  0.9420221475896133\n",
            "Running loss:  0.9420821846794845\n",
            "Running loss:  0.941970266933617\n",
            "Running loss:  0.9418115799113513\n",
            "Running loss:  0.9418164847104327\n",
            "Running loss:  0.9416159350742386\n",
            "Running loss:  0.9416049100921663\n",
            "Running loss:  0.9417089414947734\n",
            "Running loss:  0.9418453634046355\n",
            "Running loss:  0.9418673964263286\n",
            "Running loss:  0.941685390282779\n",
            "Running loss:  0.9417846086648212\n",
            "Running loss:  0.9416432761916187\n",
            "Running loss:  0.9415461122897139\n",
            "Running loss:  0.9416031418604959\n",
            "Running loss:  0.9414511861280195\n",
            "Running loss:  0.9416079336802667\n",
            "Running loss:  0.9416724905964781\n",
            "Running loss:  0.9416735617011264\n",
            "Running loss:  0.9416876467731478\n",
            "Running loss:  0.9416809273879134\n",
            "Running loss:  0.9419392066036996\n",
            "Running loss:  0.9418867723714054\n",
            "Running loss:  0.9418938574314276\n",
            "Running loss:  0.9418620664048701\n",
            "Running loss:  0.9419690512440921\n",
            "Running loss:  0.9420939062473195\n",
            "Running loss:  0.9422013076812955\n",
            "Running loss:  0.9422822395900221\n",
            "Running loss:  0.9425480834862143\n",
            "Running loss:  0.9424500499468328\n",
            "Running loss:  0.9425127529118662\n",
            "Running loss:  0.9426700941616156\n",
            "Running loss:  0.9424871487346133\n",
            "Running loss:  0.9424920570613827\n",
            "Running loss:  0.9424290598530352\n",
            "Running loss:  0.9424271615360577\n",
            "Running loss:  0.9425137402839351\n",
            "Running loss:  0.9427981355449655\n",
            "Running loss:  0.9426719825220132\n",
            "Running loss:  0.9429095754748457\n",
            "Running loss:  0.9429305429190487\n",
            "Running loss:  0.9429997010593685\n",
            "Running loss:  0.9429409724603038\n",
            "Running loss:  0.9428241202757831\n",
            "Running loss:  0.9430038836960689\n",
            "Running loss:  0.9426733822563461\n",
            "Running loss:  0.9428447247324926\n",
            "Running loss:  0.9428935283696283\n",
            "Running loss:  0.9429694705064151\n",
            "Running loss:  0.9428760039932377\n",
            "Running loss:  0.9427666868397453\n",
            "Running loss:  0.9426789180979807\n",
            "Running loss:  0.9427673486511958\n",
            "Running loss:  0.9426728843320422\n",
            "Running loss:  0.9424972081455406\n",
            "Running loss:  0.9426297342645424\n",
            "Running loss:  0.9425032003194027\n",
            "Running loss:  0.9423425086210178\n",
            "Running loss:  0.9423066364167856\n",
            "Running loss:  0.9423605498547368\n",
            "Running loss:  0.9425005744179367\n",
            "Running loss:  0.9426014283215194\n",
            "Running loss:  0.9426001048987909\n",
            "Running loss:  0.9426941714057728\n",
            "Running loss:  0.942684293050118\n",
            "Running loss:  0.9424864852558396\n",
            "Running loss:  0.9424349084108082\n",
            "Running loss:  0.9425761649039857\n",
            "Running loss:  0.9424674525597352\n",
            "Running loss:  0.9425626530588808\n",
            "Running loss:  0.9425764452253313\n",
            "Running loss:  0.9427650514587483\n",
            "Running loss:  0.943022794765494\n",
            "Running loss:  0.9430960552366509\n",
            "Running loss:  0.9428823996927147\n",
            "Running loss:  0.9428739650048561\n",
            "Running loss:  0.9428035718375979\n",
            "Running loss:  0.9428891708335844\n",
            "Running loss:  0.943005685771362\n",
            "Running loss:  0.9428625573289956\n",
            "Running loss:  0.9427888991624522\n",
            "Running loss:  0.942669791798879\n",
            "Running loss:  0.9426616372363782\n",
            "Running loss:  0.9425356827126266\n",
            "Running loss:  0.9427053321343312\n",
            "Running loss:  0.9429513061438879\n",
            "Running loss:  0.9429785065317656\n",
            "Running loss:  0.9432327937960504\n",
            "Running loss:  0.9432377061457223\n",
            "Running loss:  0.9433067003125569\n",
            "Running loss:  0.9433368913719984\n",
            "Running loss:  0.9431867975477899\n",
            "Running loss:  0.943291941735338\n",
            "Running loss:  0.9432937623845641\n",
            "Running loss:  0.9431757048712204\n",
            "Running loss:  0.9431441868558059\n",
            "Running loss:  0.9432443730191399\n",
            "Running loss:  0.943206996232419\n",
            "Running loss:  0.9432354009957948\n",
            "Running loss:  0.9433097291566618\n",
            "Running loss:  0.9433432097026027\n",
            "Running loss:  0.9433701768704477\n",
            "Running loss:  0.9433191057545266\n",
            "Running loss:  0.9431742454217235\n",
            "Running loss:  0.9433220411020597\n",
            "Running loss:  0.9432922825985353\n",
            "Running loss:  0.943321346235418\n",
            "Running loss:  0.9433364607071211\n",
            "Running loss:  0.943220001947104\n",
            "Running loss:  0.9429972862721266\n",
            "Running loss:  0.9431134754999158\n",
            "Running loss:  0.9430670229270981\n",
            "Running loss:  0.9429348487425997\n",
            "Running loss:  0.9428768372958032\n",
            "Running loss:  0.9427876759553148\n",
            "Running loss:  0.9425499160361245\n",
            "Running loss:  0.9426843983162797\n",
            "Running loss:  0.9425791529207648\n",
            "Running loss:  0.9425692629953988\n",
            "Running loss:  0.9425836404799404\n",
            "Running loss:  0.9424019866692076\n",
            "Running loss:  0.9424610167061217\n",
            "Running loss:  0.9426198627960777\n",
            "Running loss:  0.9426045939010587\n",
            "Running loss:  0.9425597923264182\n",
            "Running loss:  0.9425144344489539\n",
            "Running loss:  0.9425332859272508\n",
            "Running loss:  0.94275570287908\n",
            "Running loss:  0.9427481543481494\n",
            "Running loss:  0.9428981398195581\n",
            "Running loss:  0.9429719891644505\n",
            "Running loss:  0.9427261193062278\n",
            "Running loss:  0.9428514699229583\n",
            "Running loss:  0.9429960268770162\n",
            "Running loss:  0.9430306274409833\n",
            "Running loss:  0.9428298598596279\n",
            "Running loss:  0.9428673378659612\n",
            "Running loss:  0.9428739042500316\n",
            "Running loss:  0.9427345213225155\n",
            "Running loss:  0.9428837850875381\n",
            "Running loss:  0.9428296609728859\n",
            "Running loss:  0.9425025708892467\n",
            "Running loss:  0.942561905826287\n",
            "Running loss:  0.9426378412297145\n",
            "Running loss:  0.9425274141361284\n",
            "Running loss:  0.9423119381599309\n",
            "Running loss:  0.9421541131697777\n",
            "Running loss:  0.942075224734797\n",
            "Running loss:  0.9419287331010769\n",
            "Running loss:  0.9421602847389363\n",
            "Running loss:  0.9423090327855907\n",
            "Running loss:  0.9422841452754677\n",
            "Running loss:  0.9424618990772144\n",
            "Running loss:  0.942465975296115\n",
            "Running loss:  0.942444183116225\n",
            "Running loss:  0.9423459326863534\n",
            "Running loss:  0.9423949117135411\n",
            "Running loss:  0.9422942247719435\n",
            "Running loss:  0.9423438301722451\n",
            "Running loss:  0.9423411320687788\n",
            "Running loss:  0.9423630952353552\n",
            "Running loss:  0.9422958268550569\n",
            "Running loss:  0.9422241522904997\n",
            "Running loss:  0.9423413750841103\n",
            "Running loss:  0.9424740253187129\n",
            "Running loss:  0.94247415769477\n",
            "Running loss:  0.9424600446621476\n",
            "Running loss:  0.9424200206249206\n",
            "Running loss:  0.9423840260452044\n",
            "Running loss:  0.9423303278874469\n",
            "Running loss:  0.9423217047363466\n",
            "Running loss:  0.9422059910023116\n",
            "Running loss:  0.9422127381096843\n",
            "Running loss:  0.9422997194946078\n",
            "Running loss:  0.9424645110088671\n",
            "Running loss:  0.9424624969689885\n",
            "Running loss:  0.9424019779229157\n",
            "Running loss:  0.9422823224866738\n",
            "Running loss:  0.9421674891537545\n",
            "Running loss:  0.9423671210482276\n",
            "Running loss:  0.9422664461802802\n",
            "Running loss:  0.9423612081470948\n",
            "Running loss:  0.9423902253207396\n",
            "Running loss:  0.9422372924246193\n",
            "Running loss:  0.9422677427318509\n",
            "Running loss:  0.9423017849451877\n",
            "Running loss:  0.9423058642440073\n",
            "Running loss:  0.942253156128572\n",
            "Running loss:  0.9421782136050643\n",
            "Running loss:  0.9420469579496906\n",
            "Running loss:  0.9420288425528834\n",
            "Running loss:  0.9421052139893215\n",
            "Running loss:  0.9420592238734685\n",
            "Running loss:  0.9421407738560559\n",
            "Running loss:  0.9421242972431442\n",
            "Running loss:  0.9420228425262047\n",
            "Running loss:  0.9419766226358323\n",
            "Running loss:  0.941771664282462\n",
            "Running loss:  0.9418375838581147\n",
            "Running loss:  0.941840288240939\n",
            "Running loss:  0.9416511662302606\n",
            "Running loss:  0.9415575069586813\n",
            "Running loss:  0.9414775397273963\n",
            "Running loss:  0.9418009771640725\n",
            "Running loss:  0.9417932784976466\n",
            "Running loss:  0.9417908270992387\n",
            "Running loss:  0.9418269765601749\n",
            "Running loss:  0.9417331146131361\n",
            "Running loss:  0.9417396165081156\n",
            "Running loss:  0.9417885263874248\n",
            "Running loss:  0.94175961881883\n",
            "Running loss:  0.9417433816748726\n",
            "Running loss:  0.9417708149116706\n",
            "Running loss:  0.9417846403272009\n",
            "Running loss:  0.9418465562307411\n",
            "Running loss:  0.9417484272359329\n",
            "Running loss:  0.9418698193273061\n",
            "Running loss:  0.9418848538617725\n",
            "Running loss:  0.9419810967399376\n",
            "Running loss:  0.941861688348471\n",
            "Running loss:  0.9419391606649129\n",
            "Running loss:  0.9418843026402623\n",
            "Running loss:  0.9419809068374118\n",
            "Running loss:  0.9418675701985072\n",
            "Running loss:  0.9419702422783152\n",
            "Running loss:  0.9420318431349147\n",
            "Running loss:  0.9419603752828756\n",
            "Running loss:  0.9420274136452695\n",
            "Running loss:  0.9421465198014171\n",
            "Running loss:  0.9420612756590255\n",
            "Running loss:  0.942048892071174\n",
            "Running loss:  0.9420083340419669\n",
            "Running loss:  0.9420997670777713\n",
            "Running loss:  0.9422763060817985\n",
            "Running loss:  0.9423309949935644\n",
            "Running loss:  0.9424161251868042\n",
            "Running loss:  0.9424455407247424\n",
            "Running loss:  0.94249933999198\n",
            "Running loss:  0.9425723004741132\n",
            "Running loss:  0.9425577960969982\n",
            "Running loss:  0.9427000805205961\n",
            "Running loss:  0.9427172899710378\n",
            "Running loss:  0.9426215532523651\n",
            "Running loss:  0.9426188394427294\n",
            "Running loss:  0.942527913701667\n",
            "Running loss:  0.942541295379685\n",
            "Running loss:  0.9424780043008734\n",
            "Running loss:  0.9424078835952373\n",
            "Running loss:  0.9424093828380603\n",
            "Running loss:  0.9422648468520276\n",
            "Running loss:  0.9422006804273929\n",
            "Running loss:  0.9420899597428518\n",
            "Running loss:  0.9420459230772319\n",
            "Running loss:  0.9419679934023406\n",
            "Running loss:  0.9418852612749494\n",
            "Running loss:  0.9420072132867662\n",
            "Running loss:  0.9420490432727765\n",
            "Running loss:  0.9420246929600596\n",
            "Running loss:  0.9420975818242469\n",
            "Running loss:  0.9419800856583741\n",
            "Running loss:  0.9420733167448536\n",
            "Running loss:  0.9420767173110983\n",
            "Running loss:  0.9420082071661494\n",
            "Running loss:  0.9418563144522866\n",
            "Running loss:  0.9419202519311778\n",
            "Running loss:  0.9419008753114227\n",
            "Running loss:  0.9420877217224511\n",
            "Running loss:  0.9421427737311638\n",
            "Running loss:  0.9422511416351863\n",
            "Running loss:  0.942310298614855\n",
            "Running loss:  0.94231545643039\n",
            "Running loss:  0.9423018622717778\n",
            "Running loss:  0.9423008028041256\n",
            "Running loss:  0.9422114194427184\n",
            "Running loss:  0.942150274857704\n",
            "Running loss:  0.9421077976216816\n",
            "Running loss:  0.9420479956412988\n",
            "Running loss:  0.942141102219645\n",
            "Running loss:  0.9422002591960532\n",
            "Running loss:  0.9420222031889616\n",
            "Running loss:  0.9420946287013697\n",
            "Running loss:  0.9420762959340657\n",
            "Running loss:  0.9419781571468879\n",
            "Running loss:  0.9420020548965441\n",
            "Running loss:  0.9419267159951509\n",
            "Running loss:  0.9421510768999867\n",
            "Running loss:  0.9422804374070084\n",
            "Running loss:  0.9423259866364734\n",
            "Running loss:  0.9423623837107964\n",
            "Running loss:  0.942322225575596\n",
            "Running loss:  0.9423771831185767\n",
            "Running loss:  0.9423297717844487\n",
            "Running loss:  0.942301066200136\n",
            "Running loss:  0.9422443906917128\n",
            "Running loss:  0.9422575262845964\n",
            "Running loss:  0.9422598866564224\n",
            "Running loss:  0.9422170474685436\n",
            "Running loss:  0.9423532873853955\n",
            "Running loss:  0.9423956719105585\n",
            "Running loss:  0.9424692257355473\n",
            "Running loss:  0.9425539703960963\n",
            "Running loss:  0.9427835979062599\n",
            "Running loss:  0.9428452898069712\n",
            "Running loss:  0.9428807682683569\n",
            "Running loss:  0.9428069723885018\n",
            "Running loss:  0.9427714940006284\n",
            "Running loss:  0.9428185231839127\n",
            "Running loss:  0.942718534944563\n",
            "Running loss:  0.9427159018870035\n",
            "Running loss:  0.9426939225214571\n",
            "Running loss:  0.942661456654619\n",
            "Running loss:  0.9426655531296092\n",
            "Running loss:  0.9426641455332474\n",
            "Running loss:  0.942654978677826\n",
            "Running loss:  0.9424905770741163\n",
            "Running loss:  0.9424571306004215\n",
            "Running loss:  0.9425018979725621\n",
            "Running loss:  0.9425300036522092\n",
            "Running loss:  0.942509207611861\n",
            "Running loss:  0.9425505908336131\n",
            "Running loss:  0.9426236720965006\n",
            "Running loss:  0.9426165993942519\n",
            "Running loss:  0.9425263087958369\n",
            "Running loss:  0.9424884101216819\n",
            "Running loss:  0.9424423710481068\n",
            "Running loss:  0.9423700659421019\n",
            "Running loss:  0.9424596194172428\n",
            "Running loss:  0.942466143229736\n",
            "Running loss:  0.9423845752567783\n",
            "Running loss:  0.9422312193161444\n",
            "Running loss:  0.9422410110244386\n",
            "Running loss:  0.9424382978586167\n",
            "Running loss:  0.9423973090827678\n",
            "Running loss:  0.9423583258287671\n",
            "Running loss:  0.9425979434358142\n",
            "Running loss:  0.9425684054704914\n",
            "Running loss:  0.9428081091057047\n",
            "Running loss:  0.9429386981289791\n",
            "Running loss:  0.9429774503748682\n",
            "Running loss:  0.94305192897038\n",
            "Running loss:  0.9429721921480041\n",
            "Running loss:  0.9429928615376926\n",
            "Running loss:  0.9430095506348275\n",
            "Running loss:  0.9431178813969826\n",
            "Running loss:  0.9430513546997299\n",
            "Running loss:  0.9431421863425233\n",
            "Running loss:  0.943198395254324\n",
            "Running loss:  0.9430279663149147\n",
            "Running loss:  0.9430596090304417\n",
            "Running loss:  0.9432611578746667\n",
            "Running loss:  0.9433140392957409\n",
            "Running loss:  0.9432043702780378\n",
            "Running loss:  0.9431675459778522\n",
            "Running loss:  0.9432108224684568\n",
            "Running loss:  0.9432105263247501\n",
            "Running loss:  0.9433980437487705\n",
            "Running loss:  0.9435512446495864\n",
            "Running loss:  0.9435837410610315\n",
            "Running loss:  0.9434969983987441\n",
            "Running loss:  0.9435201537005361\n",
            "Running loss:  0.9435690927411783\n",
            "Running loss:  0.943531316066288\n",
            "Running loss:  0.943457483747696\n",
            "Running loss:  0.9434719606694977\n",
            "Running loss:  0.943374445384058\n",
            "Running loss:  0.9433141324745337\n",
            "Running loss:  0.9434077138354754\n",
            "Running loss:  0.9434427027646501\n",
            "Running loss:  0.9434821768430112\n",
            "Running loss:  0.9435532360054524\n",
            "Running loss:  0.9436972901162647\n",
            "Running loss:  0.9438621315814002\n",
            "Running loss:  0.9439095905151916\n",
            "Running loss:  0.9440111780766711\n",
            "Running loss:  0.9439868096908651\n",
            "Running loss:  0.9440053839109271\n",
            "Running loss:  0.9439444180413205\n",
            "Running loss:  0.9439533761615001\n",
            "Running loss:  0.9440080969847218\n",
            "Running loss:  0.9439410715528972\n",
            "Running loss:  0.9439424616066613\n",
            "Running loss:  0.9439412726451893\n",
            "Running loss:  0.9439259326345147\n",
            "Running loss:  0.9437319977873896\n",
            "Running loss:  0.9437564565625824\n",
            "Running loss:  0.943706676305349\n",
            "Running loss:  0.943630636692563\n",
            "Running loss:  0.9436498642423938\n",
            "Running loss:  0.9437642280750715\n",
            "Running loss:  0.9439015952491598\n",
            "Running loss:  0.9439944500641875\n",
            "Running loss:  0.9438954226794989\n",
            "Running loss:  0.9439164665184836\n",
            "Running loss:  0.9440233798648906\n",
            "Running loss:  0.9439829317328301\n",
            "Running loss:  0.9439811240417478\n",
            "Running loss:  0.9440893482788608\n",
            "Running loss:  0.9441438606461344\n",
            "Running loss:  0.9442470619699797\n",
            "Running loss:  0.9442277078518957\n",
            "Running loss:  0.944107438116614\n",
            "Running loss:  0.9440630467199705\n",
            "Running loss:  0.944093298880927\n",
            "Running loss:  0.9442279541683628\n",
            "Running loss:  0.9441788972367477\n",
            "Running loss:  0.944219559448317\n",
            "Running loss:  0.9442696914383966\n",
            "Running loss:  0.9442487471407894\n",
            "Running loss:  0.9443006423649213\n",
            "Running loss:  0.9442493299647025\n",
            "Running loss:  0.9442100748805812\n",
            "Running loss:  0.9441582884330975\n",
            "Running loss:  0.9442469082933996\n",
            "Running loss:  0.9441560860673367\n",
            "Running loss:  0.9441318151155964\n",
            "Running loss:  0.9440885984016529\n",
            "Running loss:  0.9441551305517133\n",
            "Running loss:  0.9442913980541736\n",
            "Running loss:  0.9442549802659476\n",
            "Running loss:  0.9442450349450211\n",
            "Running loss:  0.9441437330327099\n",
            "Running loss:  0.9442335489162519\n",
            "Running loss:  0.9441439151209832\n",
            "Running loss:  0.9442336076617792\n",
            "Running loss:  0.9441804213576693\n",
            "Running loss:  0.944259380379772\n",
            "Running loss:  0.9442651885722005\n",
            "Running loss:  0.9441963322742024\n",
            "Running loss:  0.9442256575900421\n",
            "Running loss:  0.944159914812097\n",
            "Running loss:  0.9441413282506377\n",
            "Running loss:  0.9441857147622691\n",
            "Running loss:  0.944294289821433\n",
            "Running loss:  0.9441793507543497\n",
            "Running loss:  0.9442588447730649\n",
            "Running loss:  0.9442304154352544\n",
            "Running loss:  0.9443027936738215\n",
            "Running loss:  0.9442813690073079\n",
            "Running loss:  0.9442628905486229\n",
            "Running loss:  0.9443838166947955\n",
            "Running loss:  0.9443743888388112\n",
            "Running loss:  0.9442774415338308\n",
            "Running loss:  0.9443448490269518\n",
            "Running loss:  0.9442834819922052\n",
            "Running loss:  0.94422818050467\n",
            "Running loss:  0.9441829627380475\n",
            "Running loss:  0.9442137415821276\n",
            "Running loss:  0.9442116525093223\n",
            "Running loss:  0.944249738848579\n",
            "Running loss:  0.9440624000924287\n",
            "Running loss:  0.9440477685674118\n",
            "Running loss:  0.9440642164819272\n",
            "Running loss:  0.944087603697587\n",
            "Running loss:  0.9440047685077533\n",
            "Running loss:  0.9439537998030285\n",
            "Running loss:  0.9440494879992517\n",
            "Running loss:  0.9440753512677421\n",
            "Running loss:  0.9441797705452725\n",
            "Running loss:  0.9441654213118666\n",
            "Running loss:  0.9441098048354448\n",
            "Running loss:  0.9442247904944926\n",
            "Running loss:  0.9441744657638886\n",
            "Running loss:  0.9442020518658619\n",
            "Running loss:  0.9440748255027755\n",
            "Running loss:  0.9440448068339262\n",
            "Running loss:  0.9438712951576814\n",
            "Running loss:  0.9436381111659994\n",
            "Running loss:  0.9437541072697458\n",
            "Running loss:  0.9437423466422575\n",
            "Running loss:  0.9437032070131348\n",
            "Running loss:  0.9437301440001891\n",
            "Running loss:  0.9437817318487909\n",
            "Running loss:  0.943761285740578\n",
            "Running loss:  0.9435451116697381\n",
            "Running loss:  0.9436593784214353\n",
            "Running loss:  0.9437686073125459\n",
            "Running loss:  0.9439579245416395\n",
            "Running loss:  0.9439465647413564\n",
            "Running loss:  0.943954069557288\n",
            "Running loss:  0.9438432915293133\n",
            "Running loss:  0.9437218584874595\n",
            "Running loss:  0.9437321781191191\n",
            "Running loss:  0.9435495416665733\n",
            "Running loss:  0.943468029371637\n",
            "Running loss:  0.9434607549393801\n",
            "Running loss:  0.9435935405326772\n",
            "Running loss:  0.9435355718096197\n",
            "Running loss:  0.9434222118972985\n",
            "Running loss:  0.9435400182254587\n",
            "Running loss:  0.9434341588617453\n",
            "Running loss:  0.9433861554486952\n",
            "Running loss:  0.9433244205480645\n",
            "Running loss:  0.9433993202387466\n",
            "Running loss:  0.9433606315710109\n",
            "Running loss:  0.9434274621129243\n",
            "Running loss:  0.943410194432001\n",
            "Running loss:  0.9435205466145866\n",
            "Running loss:  0.9434096556139737\n",
            "Running loss:  0.943538452129285\n",
            "Running loss:  0.9435346004628246\n",
            "Running loss:  0.9434893672551795\n",
            "Running loss:  0.9434496502320021\n",
            "Running loss:  0.9435348992855603\n",
            "Running loss:  0.943518428429911\n",
            "Running loss:  0.9435216185912746\n",
            "Running loss:  0.9434611086186269\n",
            "Running loss:  0.9435853525365203\n",
            "Running loss:  0.9435887768069368\n",
            "Running loss:  0.9436543536194605\n",
            "Running loss:  0.9434685127968314\n",
            "Running loss:  0.9433776466259106\n",
            "Running loss:  0.9434950055047486\n",
            "Running loss:  0.9434523940023877\n",
            "Running loss:  0.9435707218489635\n",
            "Running loss:  0.9436468178534111\n",
            "Running loss:  0.9436736217228462\n",
            "Running loss:  0.9435474727600655\n",
            "Running loss:  0.9436149616306623\n",
            "Running loss:  0.9435879894752689\n",
            "Running loss:  0.9434821420479292\n",
            "Running loss:  0.9436157498947785\n",
            "Running loss:  0.9435965197753318\n",
            "Running loss:  0.9435936196892315\n",
            "Running loss:  0.9436128467806428\n",
            "Running loss:  0.9435563464155271\n",
            "Running loss:  0.9436528294083596\n",
            "Running loss:  0.9436993935352762\n",
            "Running loss:  0.9437829218059288\n",
            "Running loss:  0.9436770914865169\n",
            "Running loss:  0.9437130533833796\n",
            "Running loss:  0.943787608816876\n",
            "Running loss:  0.943860049440209\n",
            "Running loss:  0.9438575776586686\n",
            "Running loss:  0.9437568432894785\n",
            "Running loss:  0.9438425716650979\n",
            "Running loss:  0.9438214958214555\n",
            "Running loss:  0.9438428859548669\n",
            "Running loss:  0.9438225955403206\n",
            "Running loss:  0.9437080944089745\n",
            "Running loss:  0.943648962003736\n",
            "Running loss:  0.9437547628789266\n",
            "Running loss:  0.9437845234441327\n",
            "Running loss:  0.9437657734529897\n",
            "Running loss:  0.9438242961743262\n",
            "Running loss:  0.9438724902906203\n",
            "Running loss:  0.9438097513465328\n",
            "Running loss:  0.9439484462422788\n",
            "Running loss:  0.9439519340048595\n",
            "Running loss:  0.9440435210368837\n",
            "Running loss:  0.9441427474673089\n",
            "Running loss:  0.9441000703395176\n",
            "Running loss:  0.944035936174684\n",
            "Running loss:  0.9440478339698208\n",
            "Running loss:  0.9441399511466614\n",
            "Running loss:  0.944334416918282\n",
            "Running loss:  0.944238079972869\n",
            "Running loss:  0.944251133328718\n",
            "Running loss:  0.9442703879334677\n",
            "Running loss:  0.9441663068225481\n",
            "Running loss:  0.9441022555336951\n",
            "Running loss:  0.9440346533241082\n",
            "Running loss:  0.9440222337537294\n",
            "Running loss:  0.9439464345277427\n",
            "Running loss:  0.9438954519190247\n",
            "Running loss:  0.9438677059878618\n",
            "Running loss:  0.9436893802729912\n",
            "Running loss:  0.9436562874593377\n",
            "Running loss:  0.9437344745497396\n",
            "Running loss:  0.9437094969394686\n",
            "Running loss:  0.9436639046424895\n",
            "Running loss:  0.9436807845490032\n",
            "Running loss:  0.943729789397543\n",
            "Running loss:  0.9437286414006826\n",
            "Running loss:  0.9436577387151652\n",
            "Running loss:  0.9435931541071372\n",
            "Running loss:  0.9436563178304437\n",
            "Running loss:  0.9437206769705908\n",
            "Running loss:  0.9438265956152073\n",
            "Running loss:  0.9437667855180587\n",
            "Running loss:  0.943810502266516\n",
            "Running loss:  0.9437174347491003\n",
            "Running loss:  0.9437854063273127\n",
            "Running loss:  0.9437886012668706\n",
            "Running loss:  0.9437989882286107\n",
            "Running loss:  0.9437687341672127\n",
            "Running loss:  0.9436998798086835\n",
            "Running loss:  0.9436512238882286\n",
            "Running loss:  0.9436476237123506\n",
            "Running loss:  0.9436655045093015\n",
            "Running loss:  0.9436191079160112\n",
            "Running loss:  0.9435855912130314\n",
            "Running loss:  0.9434493028987012\n",
            "Running loss:  0.9433666516501361\n",
            "Running loss:  0.9433717362551082\n",
            "Running loss:  0.9434352606714832\n",
            "Running loss:  0.9434163162759203\n",
            "Running loss:  0.9433776842685945\n",
            "Running loss:  0.9432586356391376\n",
            "Running loss:  0.9431925749482245\n",
            "Running loss:  0.9433040592512055\n",
            "Running loss:  0.9433036554683997\n",
            "Running loss:  0.9433326808239725\n",
            "Running loss:  0.943223661767706\n",
            "Running loss:  0.9431841635709849\n",
            "Running loss:  0.943281718657987\n",
            "Running loss:  0.9433293637289358\n",
            "Running loss:  0.9433610983309675\n",
            "Running loss:  0.9435101649438377\n",
            "Running loss:  0.9435242367711629\n",
            "Running loss:  0.94354320734096\n",
            "Running loss:  0.943434468546539\n",
            "Running loss:  0.9435353841934674\n",
            "Running loss:  0.943464249794057\n",
            "Running loss:  0.9434307623658212\n",
            "Running loss:  0.9433354140715067\n",
            "Running loss:  0.9432257999444426\n",
            "Running loss:  0.9432707997616488\n",
            "Running loss:  0.9432006978784453\n",
            "Running loss:  0.9431491788960045\n",
            "Running loss:  0.9432798767286135\n",
            "Running loss:  0.9433258241775854\n",
            "Running loss:  0.9434311354108964\n",
            "Running loss:  0.9433390209357777\n",
            "Running loss:  0.9433033745415375\n",
            "Running loss:  0.9434048175062106\n",
            "Running loss:  0.9434229138689841\n",
            "Running loss:  0.9434406630063644\n",
            "Running loss:  0.9432865811471661\n",
            "Running loss:  0.9433080179457497\n",
            "Running loss:  0.9432452871020265\n",
            "Running loss:  0.9433101099288501\n",
            "Running loss:  0.9432909002998292\n",
            "Running loss:  0.9432640855371965\n",
            "Running loss:  0.9433169243901454\n",
            "Running loss:  0.9431926093429533\n",
            "Running loss:  0.9431164186269143\n",
            "Running loss:  0.9431621507174457\n",
            "Running loss:  0.9430998229723138\n",
            "Running loss:  0.9430439993023539\n",
            "Running loss:  0.9429708116126954\n",
            "Running loss:  0.9430328592988368\n",
            "Running loss:  0.9430801340882394\n",
            "Running loss:  0.9430698389545621\n",
            "Running loss:  0.9431924635644934\n",
            "Running loss:  0.9431254570386288\n",
            "Running loss:  0.9431646512690082\n",
            "Running loss:  0.9430972289738774\n",
            "Running loss:  0.9429628668379774\n",
            "Running loss:  0.9428470733342312\n",
            "Running loss:  0.9427802968715622\n",
            "Running loss:  0.9429083731919756\n",
            "Running loss:  0.9429418840248186\n",
            "Running loss:  0.9429191502849388\n",
            "Running loss:  0.9429812164246185\n",
            "Running loss:  0.9429520243499928\n",
            "Running loss:  0.9429175960649037\n",
            "Running loss:  0.9428501632703701\n",
            "Running loss:  0.9428555552601796\n",
            "Running loss:  0.942818411915383\n",
            "Running loss:  0.942866206182979\n",
            "Running loss:  0.9428230005832063\n",
            "Running loss:  0.9428463648512698\n",
            "Running loss:  0.9428699490156726\n",
            "Running loss:  0.942824680788457\n",
            "Running loss:  0.942804134884399\n",
            "Running loss:  0.9428764278281095\n",
            "Running loss:  0.942862323322755\n",
            "Running loss:  0.9428189462142571\n",
            "Running loss:  0.9428699989490135\n",
            "Running loss:  0.9429028787422008\n",
            "Running loss:  0.9429077811503719\n",
            "Running loss:  0.9429555063263434\n",
            "Running loss:  0.9430228406082122\n",
            "Running loss:  0.9431063247090519\n",
            "Running loss:  0.9431176293984159\n",
            "Running loss:  0.9431076083136252\n",
            "Running loss:  0.9430778997286346\n",
            "Running loss:  0.9430159552587943\n",
            "Running loss:  0.9429856734710019\n",
            "Running loss:  0.9430796695081052\n",
            "Running loss:  0.9431424483014159\n",
            "Running loss:  0.9431996943155095\n",
            "Running loss:  0.9431408390237269\n",
            "Running loss:  0.9432562170210347\n",
            "Running loss:  0.943237872045648\n",
            "Running loss:  0.9431738187133324\n",
            "Running loss:  0.9432137477716261\n",
            "Running loss:  0.9432160905169836\n",
            "Running loss:  0.9431785134009727\n",
            "Running loss:  0.9430902755525754\n",
            "Running loss:  0.9430828696254032\n",
            "Running loss:  0.9430924377560703\n",
            "Running loss:  0.9430901741868091\n",
            "Running loss:  0.943034991487039\n",
            "Running loss:  0.9429915903012271\n",
            "Running loss:  0.9430846099962392\n",
            "Running loss:  0.9430248940400283\n",
            "Running loss:  0.942978845230436\n",
            "Running loss:  0.9430057369368625\n",
            "Running loss:  0.943008719321091\n",
            "Running loss:  0.9429946497089436\n",
            "Running loss:  0.9430303233157341\n",
            "Running loss:  0.94305772682463\n",
            "Running loss:  0.9431464348307564\n",
            "Running loss:  0.9432148210455653\n",
            "Running loss:  0.9431871522776296\n",
            "Running loss:  0.9432096548891816\n",
            "Running loss:  0.9431774298946434\n",
            "Running loss:  0.9431563109012719\n",
            "Running loss:  0.9431383124250758\n",
            "Running loss:  0.9431101208231606\n",
            "Running loss:  0.943072468242063\n",
            "Running loss:  0.9431044972602726\n",
            "Running loss:  0.9430083996380094\n",
            "Running loss:  0.9429577645841435\n",
            "Running loss:  0.9429420830987213\n",
            "Running loss:  0.9428978412525548\n",
            "Running loss:  0.9428798378600671\n",
            "Running loss:  0.942853288316954\n",
            "Running loss:  0.9429467819309094\n",
            "Running loss:  0.9430553714801652\n",
            "Running loss:  0.9429930286746825\n",
            "Running loss:  0.942950252243837\n",
            "Running loss:  0.9429252049550341\n",
            "Running loss:  0.9429097833481899\n",
            "Running loss:  0.9429691483664941\n",
            "Running loss:  0.9429967383707502\n",
            "Running loss:  0.9429986086301848\n",
            "Running loss:  0.9430473252414461\n",
            "Running loss:  0.9430097601421096\n",
            "Running loss:  0.9429767545082389\n",
            "Running loss:  0.9429958288037525\n",
            "Running loss:  0.9429650721557342\n",
            "Running loss:  0.9429457971742711\n",
            "Running loss:  0.9428496920744872\n",
            "Running loss:  0.942867189902986\n",
            "Running loss:  0.9429280193528851\n",
            "Running loss:  0.9428731209391445\n",
            "Running loss:  0.9429367448557675\n",
            "Running loss:  0.9428012208619725\n",
            "Running loss:  0.9427684970724582\n",
            "Running loss:  0.9427964101848554\n",
            "Running loss:  0.9427547539109814\n",
            "Running loss:  0.9428759158446471\n",
            "Running loss:  0.9428674711155154\n",
            "Running loss:  0.9429747178748095\n",
            "Running loss:  0.9430138218984132\n",
            "Running loss:  0.9431024145058385\n",
            "Running loss:  0.9430614845672579\n",
            "Running loss:  0.942979019175156\n",
            "Running loss:  0.9429549645530643\n",
            "Running loss:  0.942920766788078\n",
            "Running loss:  0.9429054550353975\n",
            "Running loss:  0.9428970231173065\n",
            "Running loss:  0.9428091387636252\n",
            "Running loss:  0.9428042786404284\n",
            "Running loss:  0.9428051654268231\n",
            "Running loss:  0.9428804186203639\n",
            "Running loss:  0.9428035528509604\n",
            "Running loss:  0.9427709029050257\n",
            "Running loss:  0.9427497862373835\n",
            "Running loss:  0.942850819370408\n",
            "Running loss:  0.9428272110380259\n",
            "Running loss:  0.9428286104411793\n",
            "Running loss:  0.9427939939637238\n",
            "Running loss:  0.9428340433159584\n",
            "Running loss:  0.942770943175638\n",
            "Running loss:  0.9426196440969163\n",
            "Running loss:  0.9424466676813197\n",
            "Running loss:  0.942389568468691\n",
            "Running loss:  0.9423528804986169\n",
            "Running loss:  0.9424141554062101\n",
            "Running loss:  0.9423599525906237\n",
            "Running loss:  0.9422399483996529\n",
            "Running loss:  0.9421761000186937\n",
            "Running loss:  0.9421648484161811\n",
            "Running loss:  0.942178852258294\n",
            "Running loss:  0.942269997889605\n",
            "Running loss:  0.9422365349288574\n",
            "Running loss:  0.9422388552242474\n",
            "Running loss:  0.942165911699926\n",
            "Running loss:  0.9422309733579299\n",
            "Running loss:  0.9422186682836597\n",
            "Running loss:  0.9422231814858274\n",
            "Running loss:  0.9421594904407147\n",
            "Running loss:  0.9421388530063329\n",
            "Running loss:  0.9422450406168745\n",
            "Running loss:  0.9422467393168545\n",
            "Running loss:  0.9422059601343052\n",
            "Running loss:  0.9422896951856402\n",
            "Running loss:  0.9423006081016175\n",
            "Running loss:  0.9422962522618962\n",
            "Running loss:  0.9422662728012928\n",
            "Running loss:  0.9423495635009708\n",
            "Running loss:  0.9423361091347306\n",
            "Running loss:  0.9423434004446524\n",
            "Running loss:  0.9423791962525645\n",
            "Running loss:  0.9424395070143057\n",
            "Running loss:  0.9424017111107519\n",
            "Running loss:  0.9423128616641598\n",
            "Running loss:  0.9421859507312029\n",
            "Running loss:  0.9421504215133818\n",
            "Running loss:  0.9422017191056848\n",
            "Running loss:  0.942168098682312\n",
            "Running loss:  0.9421008799944633\n",
            "Running loss:  0.9420487989406887\n",
            "Running loss:  0.9420172180781001\n",
            "Running loss:  0.9420303587344642\n",
            "Running loss:  0.9420782519808883\n",
            "Running loss:  0.9421106824527812\n",
            "Running loss:  0.9420191632603634\n",
            "Running loss:  0.9419658606099653\n",
            "Running loss:  0.9419376885085343\n",
            "Running loss:  0.9418394143045895\n",
            "Running loss:  0.94182248737304\n",
            "Running loss:  0.9418267817697259\n",
            "Running loss:  0.9418158056681339\n",
            "Running loss:  0.941920703196276\n",
            "Running loss:  0.9420100007241655\n",
            "Running loss:  0.9419300224438988\n",
            "Running loss:  0.9419285763666139\n",
            "Running loss:  0.9419499321389786\n",
            "Running loss:  0.941955817059726\n",
            "Running loss:  0.9419736425346765\n",
            "Running loss:  0.9419849622161194\n",
            "Running loss:  0.9419943279178467\n",
            "Running loss:  0.9419748560871515\n",
            "Running loss:  0.9419252670231163\n",
            "Running loss:  0.9419774048958001\n",
            "Running loss:  0.941969186397993\n",
            "Running loss:  0.9419984890023461\n",
            "Running loss:  0.9420464061294311\n",
            "Running loss:  0.9420499600497544\n",
            "Running loss:  0.9420978592542426\n",
            "Running loss:  0.9421515109520819\n",
            "Running loss:  0.9422119630047707\n",
            "Running loss:  0.9422077815119567\n",
            "Running loss:  0.9421617809065413\n",
            "Running loss:  0.9423020516807668\n",
            "Running loss:  0.9422397757553742\n",
            "Running loss:  0.9422042022101532\n",
            "Running loss:  0.9420922387602727\n",
            "Running loss:  0.9421294725882505\n",
            "Running loss:  0.9421061206641331\n",
            "Running loss:  0.9420625123974752\n",
            "Running loss:  0.9420034238317616\n",
            "Running loss:  0.9421030334707945\n",
            "Running loss:  0.9421468922831097\n",
            "Running loss:  0.9421330685833629\n",
            "Running loss:  0.9421737454781298\n",
            "Running loss:  0.9421696721252809\n",
            "Running loss:  0.942101688113188\n",
            "Running loss:  0.9420995822517941\n",
            "Running loss:  0.9422691367985967\n",
            "Running loss:  0.9423589354335594\n",
            "Running loss:  0.9423566945598546\n",
            "Running loss:  0.9423757333129726\n",
            "Running loss:  0.9423474696393042\n",
            "Running loss:  0.9423870763199836\n",
            "Running loss:  0.9423073577802513\n",
            "Running loss:  0.9422468759110519\n",
            "Running loss:  0.9423130152653724\n",
            "Running loss:  0.9423112138106532\n",
            "Running loss:  0.9422886659470451\n",
            "Running loss:  0.9422501126141679\n",
            "Running loss:  0.9422542329279455\n",
            "Running loss:  0.9422112948433639\n",
            "Running loss:  0.9421499489887099\n",
            "Running loss:  0.9421317874749583\n",
            "Running loss:  0.9421579107193507\n",
            "Running loss:  0.9421014042632934\n",
            "Running loss:  0.9420903091888402\n",
            "Running loss:  0.9420477257868879\n",
            "Running loss:  0.9420251293152907\n",
            "Running loss:  0.9420355363182453\n",
            "Running loss:  0.9420887464716743\n",
            "Running loss:  0.9420517897032058\n",
            "Running loss:  0.9419147711698423\n",
            "Running loss:  0.9418678933690672\n",
            "Running loss:  0.9418191066324362\n",
            "Running loss:  0.9417200591801934\n",
            "Running loss:  0.9416845350622532\n",
            "Running loss:  0.9417146320049207\n",
            "Running loss:  0.9416973513079986\n",
            "Running loss:  0.9417266507739049\n",
            "Running loss:  0.9416937464865349\n",
            "Running loss:  0.9417482631909425\n",
            "Running loss:  0.9417955684836066\n",
            "Running loss:  0.9417292213199987\n",
            "Running loss:  0.94174784419178\n",
            "Running loss:  0.9417003554382208\n",
            "Running loss:  0.9417492457305499\n",
            "Running loss:  0.9417381541628239\n",
            "Running loss:  0.9417473127366427\n",
            "Running loss:  0.941685968751505\n",
            "Running loss:  0.941773915447728\n",
            "Running loss:  0.9417012106988496\n",
            "Running loss:  0.9417813817361187\n",
            "Running loss:  0.9418032648103439\n",
            "Running loss:  0.9417454486940536\n",
            "Running loss:  0.9416696182501378\n",
            "Running loss:  0.9416790542117679\n",
            "\n",
            "Epoch completed!\n",
            "Epoch Loss:  0.9416443313545707\n",
            "Epoch Perplexity:  2.5641943388119204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "964ec6642dbc4b09bae8ac32f3054ca5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Classification report after epoch:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.05      0.08       951\n",
            "           1       0.36      0.13      0.19      2024\n",
            "           2       0.49      0.86      0.63      2726\n",
            "\n",
            "    accuracy                           0.47      5701\n",
            "   macro avg       0.36      0.35      0.30      5701\n",
            "weighted avg       0.40      0.47      0.38      5701\n",
            "\n",
            "[[  44  167  740]\n",
            " [  66  257 1701]\n",
            " [  89  283 2354]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8m2G4XAOo9h"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}