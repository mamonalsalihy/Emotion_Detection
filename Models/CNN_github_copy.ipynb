{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_github_copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "124b3b3a51514b458a433b4f73348275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2b22c4bc8cee40ebac0fadc1ef9b0d5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_216f93e7d37241a2a7775ac3a4b04d95",
              "IPY_MODEL_4208cff6fd954e9bb346cd4985626fe2"
            ]
          }
        },
        "2b22c4bc8cee40ebac0fadc1ef9b0d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "216f93e7d37241a2a7775ac3a4b04d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fd1efb93c4d2480f8d3e60a52dddd8ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1598bd7099244b85b122d01f9c4b1b97"
          }
        },
        "4208cff6fd954e9bb346cd4985626fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa816918dc4a4f758f33aa968a419703",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 119/? [00:38&lt;00:00,  3.21it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b58c95fb9f0149979f622f759d0b8c01"
          }
        },
        "fd1efb93c4d2480f8d3e60a52dddd8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1598bd7099244b85b122d01f9c4b1b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa816918dc4a4f758f33aa968a419703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b58c95fb9f0149979f622f759d0b8c01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamonalsalihy/Emotion_Detection/blob/main/Models/CNN_github_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W8GiIqkZLtC"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import pickle as pkl\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sea\n",
        "import re\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import math\n",
        "from itertools import chain\n",
        "\n",
        "import gensim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5XyMajwNENj"
      },
      "source": [
        "# Must upload the dataset splits to google session to read the data.\r\n",
        "test_path = './test.csv'\r\n",
        "train_path = './train.csv'\r\n",
        "valid_path = './valid.csv'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLpSB6QKRyt2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd66d2dd-e126-4045-8982-cfc92aa2a082"
      },
      "source": [
        "with open(train_path, \"r\") as intrain: \n",
        "    train = pd.read_csv(intrain)\n",
        "with open(valid_path, \"r\") as indev: \n",
        "    valid = pd.read_csv(indev)\n",
        "with open(test_path, \"r\") as intest: \n",
        "    test = pd.read_csv(intest)\n",
        "train"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_id</th>\n",
              "      <th>utterance_idx</th>\n",
              "      <th>context</th>\n",
              "      <th>prompt</th>\n",
              "      <th>speaker_idx</th>\n",
              "      <th>utterance</th>\n",
              "      <th>selfeval</th>\n",
              "      <th>tags</th>\n",
              "      <th>clean_prompt</th>\n",
              "      <th>clean_utterance</th>\n",
              "      <th>speaker_label</th>\n",
              "      <th>emotion_category</th>\n",
              "      <th>Total Function Words</th>\n",
              "      <th>Total Pronouns</th>\n",
              "      <th>Personal Pronouns</th>\n",
              "      <th>First Person Singular</th>\n",
              "      <th>First Person Plural</th>\n",
              "      <th>Second Person</th>\n",
              "      <th>Third Person Singular</th>\n",
              "      <th>Third Person Plural</th>\n",
              "      <th>Impersonal Pronouns</th>\n",
              "      <th>Articles</th>\n",
              "      <th>Common Verbs</th>\n",
              "      <th>Auxiliary Verbs</th>\n",
              "      <th>Past Tense</th>\n",
              "      <th>Present Tense</th>\n",
              "      <th>Future Tense</th>\n",
              "      <th>Adverbs</th>\n",
              "      <th>Prepositions</th>\n",
              "      <th>Conjunctions</th>\n",
              "      <th>Negations</th>\n",
              "      <th>Quantifiers</th>\n",
              "      <th>Number</th>\n",
              "      <th>Swear Words</th>\n",
              "      <th>Social Processes</th>\n",
              "      <th>Family</th>\n",
              "      <th>Friends</th>\n",
              "      <th>Humans</th>\n",
              "      <th>Affective Processes</th>\n",
              "      <th>Positive Emotion</th>\n",
              "      <th>...</th>\n",
              "      <th>Perceptual Processes</th>\n",
              "      <th>See</th>\n",
              "      <th>Hear</th>\n",
              "      <th>Feel</th>\n",
              "      <th>Biological Processes</th>\n",
              "      <th>Body</th>\n",
              "      <th>Health</th>\n",
              "      <th>Sexual</th>\n",
              "      <th>Ingestion</th>\n",
              "      <th>Relativity</th>\n",
              "      <th>Motion</th>\n",
              "      <th>Space</th>\n",
              "      <th>Time</th>\n",
              "      <th>Work</th>\n",
              "      <th>Achievement</th>\n",
              "      <th>Leisure</th>\n",
              "      <th>Home</th>\n",
              "      <th>Money</th>\n",
              "      <th>Religion</th>\n",
              "      <th>Death</th>\n",
              "      <th>Assent</th>\n",
              "      <th>Nonfluencies</th>\n",
              "      <th>Fillers</th>\n",
              "      <th>Total first person</th>\n",
              "      <th>Total third person</th>\n",
              "      <th>Positive feelings</th>\n",
              "      <th>Optimism and energy</th>\n",
              "      <th>Communication</th>\n",
              "      <th>Other references to people</th>\n",
              "      <th>Up</th>\n",
              "      <th>Down</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>School</th>\n",
              "      <th>Sports</th>\n",
              "      <th>TV</th>\n",
              "      <th>Music</th>\n",
              "      <th>Metaphysical issues</th>\n",
              "      <th>Physical states and functions</th>\n",
              "      <th>Sleeping</th>\n",
              "      <th>Grooming</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>1</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>1</td>\n",
              "      <td>I remember going to see the fireworks with my ...</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>i remember going to see the fireworks with my ...</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>2</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>0</td>\n",
              "      <td>Was this a friend you were in love with_comma_...</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>was this a friend you were in love with or jus...</td>\n",
              "      <td>listener</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>3</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>1</td>\n",
              "      <td>This was a best friend. I miss her.</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>this was a best friend. i miss her.</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>4</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>0</td>\n",
              "      <td>Where has she gone?</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>where has she gone?</td>\n",
              "      <td>listener</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hit:0_conv:1</td>\n",
              "      <td>5</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I remember going to the fireworks with my best...</td>\n",
              "      <td>1</td>\n",
              "      <td>We no longer talk.</td>\n",
              "      <td>5|5|5_2|2|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i remember going to the fireworks with my best...</td>\n",
              "      <td>we no longer talk.</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76663</th>\n",
              "      <td>hit:12424_conv:24848</td>\n",
              "      <td>5</td>\n",
              "      <td>sentimental</td>\n",
              "      <td>I found some pictures of my grandma in the att...</td>\n",
              "      <td>389</td>\n",
              "      <td>Yeah reminds me of the good old days.  I miss ...</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i found some pictures of my grandma in the att...</td>\n",
              "      <td>yeah reminds me of the good old days.  i miss ...</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.333333</td>\n",
              "      <td>16.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76664</th>\n",
              "      <td>hit:12424_conv:24849</td>\n",
              "      <td>1</td>\n",
              "      <td>surprised</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>294</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>38.461538</td>\n",
              "      <td>30.769231</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76665</th>\n",
              "      <td>hit:12424_conv:24849</td>\n",
              "      <td>2</td>\n",
              "      <td>surprised</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>389</td>\n",
              "      <td>Oh hey that's awesome!  That is awesome right?</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>oh hey that's awesome!  that is awesome right?</td>\n",
              "      <td>listener</td>\n",
              "      <td>1</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>38.461538</td>\n",
              "      <td>30.769231</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76666</th>\n",
              "      <td>hit:12424_conv:24849</td>\n",
              "      <td>3</td>\n",
              "      <td>surprised</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>294</td>\n",
              "      <td>It is soooo awesome.  We have been wanting a b...</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>it is soooo awesome.  we have been wanting a b...</td>\n",
              "      <td>speaker</td>\n",
              "      <td>1</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>38.461538</td>\n",
              "      <td>30.769231</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76667</th>\n",
              "      <td>hit:12424_conv:24849</td>\n",
              "      <td>4</td>\n",
              "      <td>surprised</td>\n",
              "      <td>I woke up this morning to my wife telling me s...</td>\n",
              "      <td>389</td>\n",
              "      <td>That is awesome!!!! Congratulations!</td>\n",
              "      <td>5|5|5_5|5|5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i woke up this morning to my wife telling me s...</td>\n",
              "      <td>that is awesome!!!! congratulations!</td>\n",
              "      <td>listener</td>\n",
              "      <td>1</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>38.461538</td>\n",
              "      <td>30.769231</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>7.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76668 rows Ã— 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    conv_id  utterance_idx  ... Sleeping Grooming\n",
              "0              hit:0_conv:1              1  ...        0        0\n",
              "1              hit:0_conv:1              2  ...        0        0\n",
              "2              hit:0_conv:1              3  ...        0        0\n",
              "3              hit:0_conv:1              4  ...        0        0\n",
              "4              hit:0_conv:1              5  ...        0        0\n",
              "...                     ...            ...  ...      ...      ...\n",
              "76663  hit:12424_conv:24848              5  ...        0        0\n",
              "76664  hit:12424_conv:24849              1  ...        0        0\n",
              "76665  hit:12424_conv:24849              2  ...        0        0\n",
              "76666  hit:12424_conv:24849              3  ...        0        0\n",
              "76667  hit:12424_conv:24849              4  ...        0        0\n",
              "\n",
              "[76668 rows x 93 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR4HI3VSwdIJ"
      },
      "source": [
        "##Feature Builder Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "545JwQedwe-a"
      },
      "source": [
        "class Sequencer(object):\n",
        "    def __init__(self, tokens, bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>'):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "\n",
        "        self.pad_index = self.add_token(pad_token)\n",
        "        self.unk_index = self.add_token(unk_token) \n",
        "        self.bos_index = self.add_token(bos_token)\n",
        "        self.eos_index = self.add_token(eos_token)\n",
        "\n",
        "        for token in tokens:\n",
        "            self.add_token(token)\n",
        "\n",
        "    def add_token(self, token):\n",
        "\n",
        "        self.word2idx[token] = new_index = len(self.word2idx)\n",
        "        self.idx2word[new_index] = token\n",
        "\n",
        "        return new_index\n",
        "\n",
        "    def encode(self, tokens):\n",
        "        # Input will look like:\n",
        "        # [<s>, w1, w2, ..., wn, </s>]\n",
        "\n",
        "        sequence = [self.bos_index]\n",
        "        for token in tokens:\n",
        "\n",
        "            index = self.word2idx.get(token, self.unk_index)\n",
        "            sequence.append(index)\n",
        "        sequence.append(self.eos_index)\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def create_padded_tensor(self, sequences):\n",
        "        # Input: [[4, 2, 3], [5, 4, 2, 4, 6]]\n",
        "        # Output:\n",
        "        # Tensor\n",
        "        # 4 2 3 0 0\n",
        "        # 5 4 2 4 6\n",
        "\n",
        "        # Given a list of sequences, pad all to the same length\n",
        "        lengths = [len(sequence) for sequence in sequences]\n",
        "        max_seq_len = max(lengths)\n",
        "        tensor = torch.full((len(sequences), max_seq_len), self.pad_index, dtype=torch.long)\n",
        "\n",
        "        for i, sequence in enumerate(sequences):\n",
        "            for j, token in enumerate(sequence):\n",
        "                tensor[i][j] = token\n",
        "        return tensor\n",
        "\n",
        "# For converting labels into indices\n",
        "class LabelIndexer(object):\n",
        "    def __init__(self, labels):\n",
        "        self.label2idx = {label: i for i, label in enumerate(labels)}\n",
        "        self.idx2label = {i:label for label, i in self.label2idx.items()}\n",
        "        self.labels = labels\n",
        "\n",
        "    def encode(self, y):\n",
        "        return self.label2idx[y]\n",
        "\n",
        "    def encode_batch(self, ys):\n",
        "        return torch.LongTensor([self.encode(y) for y in ys])\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMi7614Zwf7O"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4juajTNCxYlm"
      },
      "source": [
        "class EmpatheticDataset(Dataset):\n",
        "    def __init__(self, texts, liwc, labels, input_transformer, output_transformer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.liwc  = liwc.values\n",
        "        self.input_transformer = input_transformer\n",
        "        self.output_transformer = output_transformer\n",
        "\n",
        "    def __getitem__(self, index): # Return a single example\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "        liwc = self.liwc[index]\n",
        "        x_liwc = torch.tensor(liwc)\n",
        "        x = self.input_transformer(text)\n",
        "        y = self.output_transformer(label)       \n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2zZqanbdeiK"
      },
      "source": [
        "class EmpatheticDatasetLIWC(Dataset):\r\n",
        "    def __init__(self, texts, liwc, labels, input_transformer, output_transformer):\r\n",
        "        self.texts = texts\r\n",
        "        self.labels = labels\r\n",
        "        self.liwc  = liwc.values\r\n",
        "        self.input_transformer = input_transformer\r\n",
        "        self.output_transformer = output_transformer\r\n",
        "\r\n",
        "    def __getitem__(self, index): # Return a single example\r\n",
        "        text = self.texts[index]\r\n",
        "        label = self.labels[index]\r\n",
        "        liwc = self.liwc[index]\r\n",
        "        x_liwc = torch.tensor(liwc)\r\n",
        "        x = self.input_transformer(text)\r\n",
        "        y = self.output_transformer(label)       \r\n",
        "        return x_liwc.float(), y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.texts)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb728zcXx2Nd"
      },
      "source": [
        "## Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iTOIdN2x5vv"
      },
      "source": [
        "class MultiClassTrainer(object):\n",
        "    \"\"\"\n",
        "    Trainer for training a multi-class classification model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, device=\"cpu\", log_every_n=None):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        self.log_every_n = log_every_n if log_every_n else 0\n",
        "\n",
        "\n",
        "    def _print_summary(self):\n",
        "        print(self.model)\n",
        "        print(self.optimizer)\n",
        "        print(self.loss_fn)\n",
        "\n",
        "    def train(self, loader):\n",
        "        \"\"\"\n",
        "        Run a single epoch of training\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.train() # Run model in training mode\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        for i, batch in tqdm(enumerate(loader)):\n",
        "            batch_size = batch[0].shape[0]\n",
        "            self.optimizer.zero_grad() # Always set gradient to 0 before computing it\n",
        "\n",
        "            logits = self.model(batch[0].to(self.device)) # Forward pass, # Wx + b\n",
        "            #print(logits)\n",
        "            #print(batch[1].view(-1).to(self.device))\n",
        "            loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss: Cross entropy loss\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            \n",
        "\n",
        "            running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "\n",
        "            if self.log_every_n and i % self.log_every_n == 0:\n",
        "                print(\"Running loss: \", running_loss)\n",
        "\n",
        "            running_loss_history.append(running_loss)\n",
        "\n",
        "            loss.backward() # Perform backprop, which will compute dL/dw\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 3.0)\n",
        "            self.optimizer.step() # Update step: w = w - eta * dL / dW\n",
        "\n",
        "        print(\"Epoch completed!\")\n",
        "        print(\"Epoch Loss: \", running_loss)\n",
        "        print(\"Epoch Perplexity: \", math.exp(running_loss))\n",
        "\n",
        "        # The history information can allow us to draw a loss plot\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def evaluate(self, loader, labels):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a validation set\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval() # Run model in eval mode (disables dropout layer)\n",
        "\n",
        "        batch_wise_true_labels = []\n",
        "        batch_wise_predictions = []\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient computation - required only during training\n",
        "            for i, batch in tqdm(enumerate(loader)):\n",
        "                # batch[0] shape: (batch_size, input_size)\n",
        "\n",
        "                logits = self.model(batch[0].to(self.device)) # Run forward pass (except we don't store gradients)\n",
        "                # logits shape: (batch_size, num_classes)\n",
        "                \n",
        "                loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss\n",
        "                # No backprop is done during validation\n",
        "                loss_history.append(loss.item())\n",
        "\n",
        "                running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "                \n",
        "                running_loss_history.append(running_loss)\n",
        "\n",
        "                # Converts the raw outputs into probabilities for each class using softmax\n",
        "                probs = F.softmax(logits, dim=-1) \n",
        "                # probs shape: (batch_size, num_classes)\n",
        "\n",
        "                predictions = torch.argmax(probs, dim=-1) # Output predictions\n",
        "                # predictions shape: (batch_size)\n",
        "\n",
        "                batch_wise_true_labels.append(batch[1].tolist())\n",
        "                batch_wise_predictions.append(predictions.tolist())\n",
        "        \n",
        "        # flatten the list of predictions using itertools\n",
        "        all_true_labels = list(chain.from_iterable(batch_wise_true_labels))\n",
        "        all_predictions = list(chain.from_iterable(batch_wise_predictions))\n",
        "\n",
        "        # Now we can generate a classification report\n",
        "        print(\"Classification report after epoch:\")\n",
        "        print(classification_report(all_true_labels, all_predictions))\n",
        "        print(confusion_matrix(all_true_labels,all_predictions))\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def get_model_dict(self):\n",
        "        return self.model.state_dict()\n",
        "\n",
        "    def run_training(self, train_loader, valid_loader, labels, n_epochs=10):\n",
        "        # Useful for us to review what experiment we're running\n",
        "        # Normally, you'd want to save this to a file\n",
        "        self._print_summary()\n",
        "\n",
        "        train_losses = []\n",
        "        train_running_losses = []\n",
        "\n",
        "        valid_losses = []\n",
        "        valid_running_losses = []\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            loss_history, running_loss_history = self.train(train_loader)\n",
        "            valid_loss_history, valid_running_loss_history = self.evaluate(valid_loader, labels)\n",
        "\n",
        "            train_losses.append(loss_history)\n",
        "            train_running_losses.append(running_loss_history)\n",
        "\n",
        "            valid_losses.append(valid_loss_history)\n",
        "            valid_running_losses.append(valid_running_loss_history)\n",
        "\n",
        "        # Training done, let's look at the loss curves\n",
        "        all_train_losses = list(chain.from_iterable(train_losses))\n",
        "        all_train_running_losses = list(chain.from_iterable(train_running_losses))\n",
        "\n",
        "        all_valid_losses = list(chain.from_iterable(valid_losses))\n",
        "        all_valid_running_losses = list(chain.from_iterable(valid_running_losses))\n",
        "\n",
        "        train_epoch_idx = range(len(all_train_losses))\n",
        "        valid_epoch_idx = range(len(all_valid_losses))\n",
        "        # sns.lineplot(epoch_idx, all_losses)\n",
        "        sns.lineplot(x=train_epoch_idx, y=all_train_running_losses)\n",
        "        sns.lineplot(x=valid_epoch_idx, y=all_valid_running_losses)\n",
        "        plt.show()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ApvqMmyZf9"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5S8ZV6hzquO"
      },
      "source": [
        "def convert_column(dataset, column): \n",
        "   texts = [text for text in dataset[column]]\n",
        "   texts = (*texts,)\n",
        "   return texts\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f5V8vsj9Vbd"
      },
      "source": [
        "listener_train = train.loc[train[\"speaker_label\"] == \"listener\"]\r\n",
        "speaker_train = train.loc[train[\"speaker_label\"] == \"speaker\"]\r\n",
        "\r\n",
        "listener_valid = valid.loc[valid[\"speaker_label\"] == \"listener\"]\r\n",
        "speaker_valid = valid.loc[valid[\"speaker_label\"] == \"speaker\"]\r\n",
        "\r\n",
        "listener_test = test.loc[train[\"speaker_label\"] == \"listener\"]\r\n",
        "speaker_test = test.loc[train[\"speaker_label\"] == \"speaker\"]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8RgXqhT1MrP"
      },
      "source": [
        "used to test non punctuation separation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o33KNEGzjzy"
      },
      "source": [
        "train_liwc = train[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "valid_liwc = valid[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "test_liwc = test[['Total Function Words', 'Total Pronouns', 'Personal Pronouns', 'First Person Singular', 'First Person Plural', 'Second Person', 'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', 'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense', 'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers', 'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', 'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger', 'Sadness', 'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative', 'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', 'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', 'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', 'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', 'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy', 'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', 'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', 'Sleeping', 'Grooming']]\n",
        "\n",
        "\n",
        "# prompts\n",
        "train_prompt = convert_column(train,'clean_prompt')\n",
        "valid_prompt = convert_column(valid,'clean_prompt')\n",
        "test_prompt = convert_column(test,'clean_prompt')\n",
        "\n",
        "# utterances\n",
        "train_utterance = convert_column(train,'clean_utterance')\n",
        "valid_utterance = convert_column(valid,'clean_utterance')\n",
        "test_utterance = convert_column(test,'clean_utterance')\n",
        "\n",
        "# 32 emotions\n",
        "train_context_labels = convert_column(train,'context')\n",
        "valid_context_labels = convert_column(valid,'context')\n",
        "test_context_labels = convert_column(test,'context')\n",
        "\n",
        "# 3 emotions\n",
        "train_category_labels = convert_column(train,'emotion_category')\n",
        "valid_category_labels = convert_column(valid, \"emotion_category\")\n",
        "test_category_labels = convert_column(test, \"emotion_category\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VyDi84tiuyj"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cuSq_TMSjm0"
      },
      "source": [
        "#Only run when testing on PROMPTS. Do not run while testing liwc features.\n",
        "# Removing duplicates\n",
        "train = train.drop_duplicates(subset=['clean_prompt'])\n",
        "valid = valid.drop_duplicates(subset=['clean_prompt'])\n",
        "test = test.drop_duplicates(subset=['clean_prompt'])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkdgomE0zAZ4"
      },
      "source": [
        "def flatten_list(labels):\n",
        "  return list(set([tag for tag_seq in labels for tag in tag_seq.split()]))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OVXsoxfz0iF"
      },
      "source": [
        "train_prompt_flattened = flatten_list(train_prompt)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbBEppacbNEY"
      },
      "source": [
        "# Prompts Dataloader w/ 32 labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQo6N-YTlgRP"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_context_labels+valid_context_labels)))\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\n",
        "\n",
        "# embeddings\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\n",
        "\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\n",
        "\n",
        "def prepare_batch(batch, sequencer):\n",
        "    # batch: [batch_len, (text, label)]\n",
        "    texts, labels = zip(*batch)\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\n",
        "    return (text_tensor, torch.stack(labels))\n",
        "\n",
        "\n",
        "# wrapper class without liwc features\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\n",
        "\n",
        "# wrapper class with liwc features\n",
        "train_sequence_dataset = EmpatheticDataset(train_prompt, train_liwc, train_context_labels, sequence_input_transformer_, output_transformer)\n",
        "valid_sequence_dataset = EmpatheticDataset(valid_prompt, valid_liwc,  valid_context_labels, sequence_input_transformer_, output_transformer)\n",
        "test_sequence_dataset = EmpatheticDataset(test_prompt, test_liwc, test_context_labels, sequence_input_transformer_, output_transformer)\n",
        "\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=64,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmKD503pbaLQ"
      },
      "source": [
        "# Utterance Dataloader w/ 32 labels\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQb444DpbZiy"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_context_labels+valid_context_labels)))\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\n",
        "\n",
        "# embeddings\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\n",
        "\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\n",
        "\n",
        "def prepare_batch(batch, sequencer):\n",
        "    # batch: [batch_len, (text, label)]\n",
        "    texts, labels = zip(*batch)\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\n",
        "    return (text_tensor, torch.stack(labels))\n",
        "\n",
        "\n",
        "# wrapper class without liwc features\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\n",
        "\n",
        "# wrapper class with liwc features\n",
        "train_sequence_dataset = EmpatheticDataset(train_prompt, train_liwc, train_context_labels, sequence_input_transformer_, output_transformer)\n",
        "valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_liwc,  valid_context_labels, sequence_input_transformer_, output_transformer)\n",
        "test_sequence_dataset = EmpatheticDataset(test_utterance, test_liwc, test_context_labels, sequence_input_transformer_, output_transformer)\n",
        "\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=5, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=5,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJtTUgldMAO"
      },
      "source": [
        "# LIWC Dataloader w/ 32 labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD6o-tyzd7BE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4d7eea-ada1-47cd-9244-7b8494b7f16e"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_context_labels+valid_context_labels)))\r\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\r\n",
        "\r\n",
        "# embeddings\r\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\r\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\r\n",
        "\r\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\r\n",
        "\r\n",
        "def prepare_batch(batch, sequencer):\r\n",
        "    # batch: [batch_len, (text, label)]\r\n",
        "    texts, labels = zip(*batch)\r\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\r\n",
        "    return (text_tensor, torch.stack(labels))\r\n",
        "\r\n",
        "\r\n",
        "# wrapper class without liwc features\r\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\r\n",
        "\r\n",
        "# wrapper class with liwc features\r\n",
        "train_sequence_dataset = EmpatheticDatasetLIWC(train_prompt, train_liwc, train_context_labels, sequence_input_transformer_, output_transformer)\r\n",
        "valid_sequence_dataset = EmpatheticDatasetLIWC(valid_prompt, valid_liwc,  valid_context_labels, sequence_input_transformer_, output_transformer)\r\n",
        "test_sequence_dataset = EmpatheticDatasetLIWC(test_prompt, test_liwc, test_context_labels, sequence_input_transformer_, output_transformer)\r\n",
        "\r\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=16, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=16, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=16,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "\r\n",
        "len(label_indexer.idx2label)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3-lslox1biT"
      },
      "source": [
        "# Prompts Dataloader w/ 3 grouped labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdSOnYQo1apE"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_category_labels+valid_category_labels)))\r\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\r\n",
        "\r\n",
        "# embeddings\r\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\r\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\r\n",
        "\r\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\r\n",
        "\r\n",
        "def prepare_batch(batch, sequencer):\r\n",
        "    # batch: [batch_len, (text, label)]\r\n",
        "    texts, labels = zip(*batch)\r\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\r\n",
        "    return (text_tensor, torch.stack(labels))\r\n",
        "\r\n",
        "\r\n",
        "# wrapper class without liwc features\r\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\r\n",
        "\r\n",
        "# wrapper class with liwc features\r\n",
        "train_sequence_dataset = EmpatheticDataset(train_prompt, train_liwc, train_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "valid_sequence_dataset = EmpatheticDataset(valid_prompt, valid_liwc,  valid_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "test_sequence_dataset = EmpatheticDataset(test_prompt, test_liwc, test_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "\r\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=64,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9rU1JfdUOKb"
      },
      "source": [
        "# Utterance Dataloader w/ 3 grouped labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BI41q-IUcNj"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_category_labels+valid_category_labels)))\r\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\r\n",
        "\r\n",
        "# embeddings\r\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\r\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\r\n",
        "\r\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\r\n",
        "\r\n",
        "def prepare_batch(batch, sequencer):\r\n",
        "    # batch: [batch_len, (text, label)]\r\n",
        "    texts, labels = zip(*batch)\r\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\r\n",
        "    return (text_tensor, torch.stack(labels))\r\n",
        "\r\n",
        "\r\n",
        "# wrapper class without liwc features\r\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\r\n",
        "\r\n",
        "# wrapper class with liwc features\r\n",
        "train_sequence_dataset = EmpatheticDataset(train_prompt, train_liwc, train_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_liwc,  valid_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "test_sequence_dataset = EmpatheticDataset(test_utterance, test_liwc, test_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "\r\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=64, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=5, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=5,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJSMyDeUomW"
      },
      "source": [
        "#LIWC w/ 3 grouped labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46JVvl9FUu8j"
      },
      "source": [
        "label_indexer = LabelIndexer(list(set(train_context_labels+valid_context_labels)))\r\n",
        "output_transformer = lambda label: torch.LongTensor([label_indexer.encode(label)])\r\n",
        "\r\n",
        "# embeddings\r\n",
        "sequencer_ = Sequencer(train_prompt_flattened)\r\n",
        "sequence_input_transformer_ = lambda text: sequencer_.encode(text)\r\n",
        "\r\n",
        "sequence_input_transformer = lambda text: sequencer.encode(text)\r\n",
        "\r\n",
        "def prepare_batch(batch, sequencer):\r\n",
        "    # batch: [batch_len, (text, label)]\r\n",
        "    texts, labels = zip(*batch)\r\n",
        "    text_tensor = sequencer.create_padded_tensor(texts)\r\n",
        "    return (text_tensor, torch.stack(labels))\r\n",
        "\r\n",
        "\r\n",
        "# wrapper class without liwc features\r\n",
        "#train_sequence_dataset = EmpatheticDataset(train_utterance, train_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#valid_sequence_dataset = EmpatheticDataset(valid_utterance, valid_grouped_labels, sequence_input_transformer_, output_transformer)\r\n",
        "#test_sequence_dataset = EmpatheticDataset(test_utterance,test_grouped_labels,sequence_input_transformer_,output_transformer)\r\n",
        "\r\n",
        "# wrapper class with liwc features\r\n",
        "train_sequence_dataset = EmpatheticDatasetLIWC(train_prompt, train_liwc, train_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "valid_sequence_dataset = EmpatheticDatasetLIWC(valid_prompt, valid_liwc,  valid_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "test_sequence_dataset = EmpatheticDatasetLIWC(test_prompt, test_liwc, test_category_labels, sequence_input_transformer_, output_transformer)\r\n",
        "\r\n",
        "train_sequence_loader = torch.utils.data.DataLoader(train_sequence_dataset, batch_size=16, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "valid_sequence_loader = torch.utils.data.DataLoader(valid_sequence_dataset, batch_size=16, collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)\r\n",
        "test_sequence_loader = torch.utils.data.DataLoader(test_sequence_dataset,batch_size=16,collate_fn=lambda batch: prepare_batch(batch, sequencer_),shuffle=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU4-LPelxufo"
      },
      "source": [
        "class TextConvolver(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, kernel_sizes, channel_size=32, dropout=False, dropout_p=0.1, embedding_dim=128):\n",
        "        super(TextConvolver, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.add_dropout = dropout\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Define an iterable set of parallel layers which are given the same input\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, channel_size, kernel_size) for kernel_size in kernel_sizes])\n",
        "\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * channel_size, output_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x size: [batch_size, seq_len]\n",
        "\n",
        "        embed = self.embedding(x).transpose(1, 2)\n",
        "\n",
        "        convs = [F.relu(conv(embed)) for conv in self.convs]\n",
        "        # [num_filters, (batch_size, out_dim, seq_len)]\n",
        "\n",
        "        maxs = [F.max_pool1d(conv_out, conv_out.shape[2]).squeeze(2) for conv_out in convs] # Max pool across time\n",
        "        # After max pooling: [num_filters, (batch_size, channel_size, 1)]\n",
        "        # After squeezing: [num_filters, (batch_size, channel_size)]\n",
        "\n",
        "        flattened_maxs = torch.cat(maxs, dim=1)\n",
        "        # [batch_size, num_filters * channel_size]\n",
        "\n",
        "\n",
        "        # logits shape: [batch_size, output_size]\n",
        "        logits = self.fc(self.dropout(flattened_maxs))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_IpI1F9S5Vz"
      },
      "source": [
        "# To print entire confusion matrix\n",
        "import sys\n",
        "import numpy\n",
        "numpy.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG222pIdw_sj"
      },
      "source": [
        "Run with LIWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odSIWNS-nk30",
        "outputId": "0fee871a-3cc6-4272-df63-737f8d8dd7ef"
      },
      "source": [
        "vocab_size =  len(train_liwc.columns)\r\n",
        "vocab_size"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W-FGPH1xBt5"
      },
      "source": [
        "Run with Utterances and Prompts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feuPIRw-xCFt",
        "outputId": "37df9d2f-3348-47ad-9fbb-8d46ec354342"
      },
      "source": [
        "vocab_size = len(sequencer_.idx2word)\r\n",
        "print(vocab_size)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJT1IvLgxJQH"
      },
      "source": [
        "Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780,
          "referenced_widgets": [
            "124b3b3a51514b458a433b4f73348275",
            "2b22c4bc8cee40ebac0fadc1ef9b0d5d",
            "216f93e7d37241a2a7775ac3a4b04d95",
            "4208cff6fd954e9bb346cd4985626fe2",
            "fd1efb93c4d2480f8d3e60a52dddd8ff",
            "1598bd7099244b85b122d01f9c4b1b97",
            "aa816918dc4a4f758f33aa968a419703",
            "b58c95fb9f0149979f622f759d0b8c01"
          ]
        },
        "id": "VlPiFxkIliPa",
        "outputId": "d84f636c-fee1-448a-84e9-88929d66a91b"
      },
      "source": [
        "output_size = len(label_indexer.idx2label)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "cnn = TextConvolver(vocab_size, output_size, [3, 4, 5], channel_size=100, dropout_p=0.5)\n",
        "cnn_optimizer = optim.Adam(cnn.parameters(), lr=1e-3)\n",
        "\n",
        "cnn_trainer = MultiClassTrainer(cnn, cnn_optimizer, loss_fn, device=device, log_every_n=5)\n",
        "cnn_trainer.run_training(train_sequence_loader, valid_sequence_loader, label_indexer.labels, n_epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextConvolver(\n",
            "  (embedding): Embedding(14889, 128)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(128, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(128, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(128, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
            ")\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "CrossEntropyLoss()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "124b3b3a51514b458a433b4f73348275",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss:  1.2313185930252075\n",
            "Running loss:  1.3023496071497598\n",
            "Running loss:  1.2487959319894966\n",
            "Running loss:  1.193344108760357\n",
            "Running loss:  1.1703558280354454\n",
            "Running loss:  1.1699293645528646\n",
            "Running loss:  1.1578399250584264\n",
            "Running loss:  1.1326974795924294\n",
            "Running loss:  1.1307853954594311\n",
            "Running loss:  1.1135295318520586\n",
            "Running loss:  1.1043925413898394\n",
            "Running loss:  1.0948494108659883\n",
            "Running loss:  1.0920187850467495\n",
            "Running loss:  1.0855693085627123\n",
            "Running loss:  1.079608842520647\n",
            "Running loss:  1.0762835450862585\n",
            "Running loss:  1.0699211108831712\n",
            "Running loss:  1.0611851645070456\n",
            "Running loss:  1.0564693998504473\n",
            "Running loss:  1.0513721474756799\n",
            "Running loss:  1.0461531848010452\n",
            "Running loss:  1.0425120310963325\n",
            "Running loss:  1.0399068411406098\n",
            "Running loss:  1.0373012567388602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJuE4H9214DL"
      },
      "source": [
        "cnn_trainer.run_training(train_sequence_loader, test_sequence_loader, label_indexer.labels, n_epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8m2G4XAOo9h"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}