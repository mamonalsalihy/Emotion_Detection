{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_github_copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamonalsalihy/Emotion_Detection/blob/main/Models/SVM_github_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhdynzVgmVLp"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a3GwSs9fbA7"
      },
      "source": [
        "import  numpy as np\n",
        "import  os\n",
        "import  pandas as pd\n",
        "from    sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from    sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, classification_report\n",
        "from    sklearn.model_selection import train_test_split \n",
        "from    sklearn.preprocessing import LabelEncoder\n",
        "from    sklearn.svm import LinearSVC\n",
        "import  seaborn as sns\n",
        "import  matplotlib.pyplot as plt\n",
        "from    tqdm.auto import tqdm\n",
        "from    sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaQ0_vLcmorL"
      },
      "source": [
        "#Setup paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQG604M9iRxE"
      },
      "source": [
        "train_path = './train.csv'\n",
        "valid_path = './valid.csv'\n",
        "test_path = './test.csv'\n",
        "\n",
        "# read in data\n",
        "train = pd.read_csv(train_path)\n",
        "valid = pd.read_csv(valid_path)\n",
        "test  = pd.read_csv(test_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVHvjhx9vObJ"
      },
      "source": [
        "#SVM classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgz9QY5Z-RsC"
      },
      "source": [
        "class SVM_Classifier:\n",
        "\n",
        "  \"\"\"\n",
        "  class def for SVM classifier.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, X_train, y_train, liwc=False):\n",
        "  \n",
        "    \"\"\"\n",
        "    method for training and evalutating SVM classifier. input data\n",
        "    is expected to be textutal, and will be vectorizered using\n",
        "    tf-idf.\n",
        "      paramtrs:\n",
        "        X_train: type: iterable(str)\n",
        "          the input data to train svm on, each text sample is \n",
        "          expected to be in raw str form. \n",
        "        y_train: type: iterable(str or int)\n",
        "          the output/labels the svm to traget during\n",
        "          training.\n",
        "      return: none\n",
        "    \"\"\"\n",
        "\n",
        "    # svm and tf-idf instances\n",
        "    _svm = LinearSVC()\n",
        "    _tfidf_vec = TfidfVectorizer(sublinear_tf=True,\n",
        "                                 norm='l1',\n",
        "                                 encoding='utf-8',\n",
        "                                 stop_words=None)\n",
        "\n",
        "    # defined pipeline\n",
        "    if liwc: self.svm = _svm\n",
        "    else:    self.svm = Pipeline([('tfidf', _tfidf_vec), \n",
        "                                  ('svm', _svm)])\n",
        "  \n",
        "    # fit on provided trainining data\n",
        "    self.svm.fit(X_train, y_train)\n",
        "\n",
        "  def evaluate_svm(self, X_test, y_test, accuracy=True, \n",
        "                   confusion=True):\n",
        "  \n",
        "    \"\"\"\n",
        "    evaluate the SVM on test data. produces precision, recall,\n",
        "    f1, accuracy scores per sklearn's accuracy_report. also\n",
        "    produces a confusion matrix.\n",
        "      paramtrs:\n",
        "        X_test: type: iterable(str)\n",
        "          test data where samples are in form of a raw string.\n",
        "        y_test: type: iterable(str or int)\n",
        "          the targets of the data.\n",
        "        accuracy: type: bool\n",
        "          specify whether to produce the accuracy report.\n",
        "        confusion: type: bool\n",
        "          specify whether to produce the confusion matrix\n",
        "          on the provided test set.\n",
        "      return: none\n",
        "    \"\"\"\n",
        "\n",
        "    # predict on provided test data\n",
        "    y_test_pred = self.svm.predict(X_test)\n",
        "\n",
        "    # report performance on test data\n",
        "    if accuracy:\n",
        "      report = classification_report(y_test, y_test_pred)\n",
        "      print(report)\n",
        "\n",
        "    # produce confusion matrix on test\n",
        "    if confusion:\n",
        "      # get unique taget labels\n",
        "      unique_labels = list(set(y_test.tolist()))\n",
        "\n",
        "      # get confusion matrix\n",
        "      conf_mat =\\\n",
        "        confusion_matrix(y_test, y_test_pred, labels=unique_labels)\n",
        "         \n",
        "      # intialize heatmap with seaborn, specify axis, plot names\n",
        "      fig, _ = plt.subplots(figsize=(10,10))\n",
        "      sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "                  xticklabels=unique_labels, yticklabels=unique_labels)\n",
        "      plt.title('SVM Confusion on Utterances Test')\n",
        "      plt.ylabel('Actual')\n",
        "      plt.xlabel('Predicted')\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tykXnF1C9__"
      },
      "source": [
        "#Method for extracting text, targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn7g1lfv-Web"
      },
      "source": [
        "def extract_instances(data, in_col, out_col):\n",
        "\n",
        "  \"\"\"\n",
        "  method for extracting input and output data for SVM\n",
        "  training or eval. data is expected to be contained\n",
        "  in data frame.\n",
        "    data: type: pd.df\n",
        "      dataframe holding desired input, output data.\n",
        "    in_col: type: str\n",
        "      name of the column in provided dataframe holding\n",
        "      the input data.\n",
        "    out_col: type: str\n",
        "      name of the column in provided dataframe holding\n",
        "      the input data.\n",
        "    return:\n",
        "      in_data: type: iterable\n",
        "        the input data.\n",
        "      out_date: type: iterable\n",
        "        the outpu data.\n",
        "  \"\"\"\n",
        "\n",
        "  # filter data appropriately\n",
        "  temp = data[[in_col, out_col]].drop_duplicates().dropna()\n",
        "\n",
        "  # get in, out data\n",
        "  in_data, out_data  = temp[in_col], temp[out_col]\n",
        "\n",
        "  return in_data, out_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABkmuHtcDJPD"
      },
      "source": [
        "#SVM from prompts to 32 emotion labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcUDPCQbweKs"
      },
      "source": [
        "# extract prompts, labels data\n",
        "train_prompt, train_prompt_labels = extract_instances(train, 'clean_prompt', 'context')\n",
        "#valid_prompt, valid_prompt_labels = extract_instances(valid, 'clean_prompt', 'context')\n",
        "test_prompt, test_prompt_labels = extract_instances(test, 'clean_prompt', 'context')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPD7VMRNDVtz"
      },
      "source": [
        "# train svm \n",
        "clf = SVM_Classifier(train_prompt, train_prompt_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s67WHT32CXp8"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(valid_prompt, valid_prompt_labels)\n",
        "clf.evaluate_svm(test_prompt, test_prompt_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmM1Fx8VDg1X"
      },
      "source": [
        "#SVM from utterances to 32 emotion labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2iKOMD2Dtrk"
      },
      "source": [
        "# extract utterances, emotion label\n",
        "train_utter, train_utter_labels = extract_instances(train, 'clean_utterance', 'context')\n",
        "#valid_utter, valid_utter_labels = extract_instances(valid, 'clean_utterance', 'context')\n",
        "test_utter, test_utter_labels = extract_instances(test, 'clean_utterance', 'context')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4msqUGoEDyW"
      },
      "source": [
        "# train svm \n",
        "clf = SVM_Classifier(train_utter, train_utter_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoypM5PhERId"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(valid_utter, valid_utter_labels)\n",
        "clf.evaluate_svm(test_utter, test_utter_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHa7V3N3EuRC"
      },
      "source": [
        "#SVM from prompts to sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Prwvu6E8qD"
      },
      "source": [
        "# extract prompt, sentiment from data\n",
        "train_prompt, train_sent = extract_instances(train, 'clean_prompt', 'emotion_category')\n",
        "#valid_prompt, valid_sent = extract_instances(valid, 'clean_prompt', 'emotion_category')\n",
        "test_prompt, test_sent = extract_instances(test, 'clean_prompt', 'emotion_category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpgMx8bGGCSm"
      },
      "source": [
        "# train svm \n",
        "clf = SVM_Classifier(train_prompt, train_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-VdbydSGJGM"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(valid_prompt, valid_sent)\n",
        "clf.evaluate_svm(test_prompt, test_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JReAHLn4G92v"
      },
      "source": [
        "#SVM from utterances to sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwYEFSBmHA2W"
      },
      "source": [
        "# extract utterances, sentiment from data\n",
        "train_utter, train_sent = extract_instances(train, 'clean_utterance', 'emotion_category')\n",
        "#valid_utter, valid_sent = extract_instances(valid, 'clean_utter', 'emotion_category')\n",
        "test_utter, test_sent = extract_instances(test, 'clean_utterance', 'emotion_category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICt-XGkOHTO2"
      },
      "source": [
        "# train svm \n",
        "clf = SVM_Classifier(train_utter, train_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KMNZDZdHYGm"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(valid_utter, valid_sent)\n",
        "clf.evaluate_svm(test_utter, test_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBT2x99IUo7"
      },
      "source": [
        "#SVM from speaker utterances to emotion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtZuXXGvI07m"
      },
      "source": [
        "# extract spekaer data\n",
        "train_speaker = train[train[\"speaker_label\"] == \"speaker\"]\n",
        "valid_speaker = valid[valid[\"speaker_label\"] == \"speaker\"]\n",
        "test_speaker  = test[test[\"speaker_label\"] == \"speaker\"]\n",
        "\n",
        "# extract text, labels\n",
        "train_utter, train_utter_labels = extract_instances(train_speaker, 'clean_utterance', 'context')\n",
        "#valid_utter, valid_utter_labels = extract_instances(valid_speaker, 'clean_utterance', 'context')\n",
        "test_utter, test_utter_labels = extract_instances(test_speaker, 'clean_utterance', 'context')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs2v250CJYOe"
      },
      "source": [
        "# train svm \n",
        "clf = SVM_Classifier(train_utter, train_utter_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajni5R5RJbAZ"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(valid_utter, valid_utter_labels)\n",
        "clf.evaluate_svm(test_utter, test_utter_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLjKu3ceKQ2G"
      },
      "source": [
        "#SVM from listerner utterances to emotion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_rhebAdKWc3"
      },
      "source": [
        "# extract listener data\n",
        "train_listener = train[train[\"speaker_label\"] == \"listener\"]\n",
        "valid_listener = valid[valid[\"speaker_label\"] == \"listener\"]\n",
        "test_listener  = test[test[\"speaker_label\"] == \"listener\"]\n",
        "\n",
        "# extract text, labels\n",
        "train_utter, train_utter_labels = extract_instances(train_listener, 'clean_utterance', 'context')\n",
        "#valid_utter, valid_utter_labels = extract_instances(valid_listener, 'clean_utterance', 'context')\n",
        "test_utter, test_utter_labels = extract_instances(test_listener, 'clean_utterance', 'context')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yPy_L8XKg0-"
      },
      "source": [
        "# train svm \n",
        "clf = SVM_Classifier(train_utter, train_utter_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RycBCQ2KuFt"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(valid_utter, valid_utter_labels)\n",
        "clf.evaluate_svm(test_utter, test_utter_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXxoHq37K_4I"
      },
      "source": [
        "#SVM from listener utterances to sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykPCu0rvLIih"
      },
      "source": [
        "# extract listener data\n",
        "train_listener = train[train[\"speaker_label\"] == \"listener\"]\n",
        "valid_listener = valid[valid[\"speaker_label\"] == \"listener\"]\n",
        "test_listener  = test[test[\"speaker_label\"] == \"listener\"]\n",
        "\n",
        "# extract text, labels\n",
        "train_utter, train_sent = extract_instances(train_listener, 'clean_utterance', 'emotion_category')\n",
        "#valid_utter, valid_sent = extract_instances(valid_listener, 'clean_utterance', 'emotion_category')\n",
        "test_utter, test_sent = extract_instances(test_listener, 'clean_utterance', 'emotion_category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRKYq81dLUX6"
      },
      "source": [
        "# train svm \n",
        "clf = SVM_Classifier(train_utter, train_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_ti0tPrLVAY"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(valid_utter, valid_sent)\n",
        "clf.evaluate_svm(test_utter, test_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akEO9bC9LuPd"
      },
      "source": [
        "#SVM from speaker utterances to sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtANZMe_MBt0"
      },
      "source": [
        "# extract listener data\n",
        "train_listener = train[train[\"speaker_label\"] == \"listener\"]\n",
        "valid_listener = valid[valid[\"speaker_label\"] == \"listener\"]\n",
        "test_listener  = test[test[\"speaker_label\"] == \"listener\"]\n",
        "\n",
        "# extract text, labels\n",
        "train_utter, train_sent = extract_instances(train_listener, 'clean_utterance', 'emotion_category')\n",
        "#valid_utter, valid_sent = extract_instances(valid_listener, 'clean_utterance', 'emotion_category')\n",
        "test_utter, test_sent = extract_instances(test_listener, 'clean_utterance', 'emotion_category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seMLU90hMTRi"
      },
      "source": [
        "# train svm \n",
        "clf = SVM_Classifier(train_utter, train_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJwY_r0sMWW9"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(valid_utter, valid_sent)\n",
        "clf.evaluate_svm(test_utter, test_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIJ5CkP_SUOt"
      },
      "source": [
        "#SVM from LIWC to emotions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJorScIqSqNW"
      },
      "source": [
        "non_liwc = 'conv_id utterance_idx\tprompt\tspeaker_idx\tutterance\tselfeval\ttags\tclean_prompt\tclean_utterance\tspeaker_label'.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve9ykC4KS0Xw"
      },
      "source": [
        "train_liwc = train.drop(columns=non_liwc).dropna()\n",
        "valid_liwc = valid.drop(columns=non_liwc).dropna()\n",
        "test_liwc  = test.drop(columns=non_liwc).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsljrU_0T3b1"
      },
      "source": [
        "train_labels = train_liwc.context; train_liwc_feat = train_liwc.drop(columns=['context'])#.values.tolist()\n",
        "#valid_labels = valid_liwc.context; valid_liwc_feat = valid_liwc.drop(columns=['context'])\n",
        "test_labels = test_liwc.context; test_liwc_feat = test_liwc.drop(columns=['context'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E2gs3-cSW6oN",
        "outputId": "f04636c6-3053-4ba3-9483-ef36c0447942"
      },
      "source": [
        " # train svm \n",
        "clf = SVM_Classifier(train_liwc_feat, train_labels, liwc=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-158-d0b1739befa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train svm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM_Classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_liwc_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliwc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-155-036df439b4d3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_train, y_train, liwc)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# fit on provided trainining data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   def evaluate_svm(self, X_test, y_test, accuracy=True, \n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    940\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu383Y6zXAuX",
        "outputId": "c3aa0c9b-de1c-4096-8e86-880c78bf7196"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(test_liwc_feat, valid_labels)\n",
        "clf.evaluate_svm(test_liwc_feat, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      afraid       0.05      0.14      0.08        63\n",
            "       angry       0.28      0.11      0.16       451\n",
            "     annoyed       0.01      0.05      0.02        42\n",
            "anticipating       0.52      0.51      0.51       172\n",
            "     anxious       0.11      0.31      0.16        59\n",
            "apprehensive       0.09      0.30      0.14        47\n",
            "     ashamed       0.02      0.11      0.04        28\n",
            "      caring       0.02      1.00      0.04         4\n",
            "   confident       0.43      0.29      0.34       254\n",
            "     content       0.32      0.28      0.30       193\n",
            "  devastated       0.12      0.14      0.13       133\n",
            "disappointed       0.00      0.00      0.00         5\n",
            "   disgusted       0.06      0.61      0.11        18\n",
            " embarrassed       0.01      0.12      0.01         8\n",
            "     excited       0.11      0.44      0.18        52\n",
            "    faithful       0.32      0.47      0.39        80\n",
            "     furious       0.13      0.05      0.07       403\n",
            "    grateful       0.06      0.42      0.11        33\n",
            "      guilty       0.62      0.06      0.11      1556\n",
            "     hopeful       0.07      0.55      0.12        22\n",
            "   impressed       0.14      0.27      0.19        92\n",
            "     jealous       0.13      0.11      0.12       209\n",
            "      joyful       0.03      0.15      0.05        33\n",
            "      lonely       0.06      0.69      0.12        16\n",
            "   nostalgic       0.79      0.52      0.63       262\n",
            "    prepared       0.61      0.44      0.51       243\n",
            "       proud       0.37      0.30      0.33       271\n",
            "         sad       0.15      0.31      0.20        98\n",
            " sentimental       0.37      0.18      0.24       427\n",
            "   surprised       0.20      0.26      0.22       222\n",
            "   terrified       0.10      0.33      0.15        46\n",
            "    trusting       0.30      0.28      0.29       159\n",
            "\n",
            "    accuracy                           0.20      5701\n",
            "   macro avg       0.21      0.31      0.19      5701\n",
            "weighted avg       0.39      0.20      0.22      5701\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dFqDwynaOil"
      },
      "source": [
        "#SVM from LIWC features to sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nZQV5hJaYsG"
      },
      "source": [
        "non_liwc = 'conv_id utterance_idx\tprompt\tspeaker_idx\tutterance\tselfeval\ttags\tclean_prompt\tclean_utterance\tspeaker_label'.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-b3sW2vNGfO"
      },
      "source": [
        "train_liwc = train.drop(columns=non_liwc).dropna()\n",
        "valid_liwc = valid.drop(columns=non_liwc).dropna()\n",
        "test_liwc  = test.drop(columns=non_liwc).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdNcXJIaasHz"
      },
      "source": [
        "train_labels = train_liwc['emotion_category']; train_liwc_feat = train_liwc.drop(columns=['emotion_category']).values.tolist()\n",
        "#valid_labels = valid_liwc['emotion_category']; valid_liwc_feat = valid_liwc.drop(columns=['emotion_category'])\n",
        "test_labels = test_liwc['emotion_category']; test_liwc_feat = test_liwc.drop(columns=['emotion_category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_mjjTKvdzCh"
      },
      "source": [
        " # train svm \n",
        "clf = SVM_Classifier(train_liwc_feat, train_labels, liwc=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLjE49rnd7-l"
      },
      "source": [
        "# evalute it\n",
        "#clf.evaluate_svm(test_liwc_feat, valid_labels)\n",
        "clf.evaluate_svm(test_liwc_feat, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}